<h1><center>A Review on an Existing Paper and a Implementation Architecture of the QA System</center></h1>
<p>
This deliverable includes a review on paper <i>Wang, Shuohang, and Jing Jiang. "Machine comprehension using match-lstm and answer pointer." arXiv preprint arXiv:1608.07905 (2016).</i> and a implementation architecture of the paper.
<p>



<p>For review on the paper and the implementation architecture, please refer to Section 5 in <a href="folder_pdf/CS297Report.pdf">CS297Report [PDF]</a></p>






<h1><center>Word Embedding of Corpus</center></h1>
<p>
This deliverable aims at the old topic, which is replaced by the current topic in November, 2017.
<p>
<p>
I implemented NPLM algorithm in paper<i> Bengio, Yoshua, et al. "A neural probabilistic language model." Journal of machine learning research 3.Feb (2003): 1137-1155. </i>, and  skip-gram together with negative sampling in paper <i>Mikolov, Tomas, et al. "Efficient estimation of word representations in vector space." arXiv preprint arXiv:1301.3781 (2013).</i>.
</p>

<p>
The corpus I use is a collection of 284899 classic Chinese poems . I get raw data from  <a href="http://homepages.inf.ed.ac.uk/mlap/Data/EMNLP14/"> Link to raw data. </a>.  The data is published by authors of paper "Zhang, Xingxing, and Mirella Lapata. "Chinese Poetry Generation with Recurrent Neural Networks." EMNLP. 2014."  All data is split into train, valid, and test. The valid loss is the indicator for choosing model. The test loss is a cross entropy  intrinsic evaluation.  I also use word similarity as another intrinsic evaluation.
</p>

<p>
I used Python and Tensorflow to implement the model. </a>
</P>

<p>For review on NPLM algorithm and Skip-gram and experimental results, please refer to Section 3 in <a href="folder_pdf/CS297Report.pdf">CS297Report [PDF]</a></p>


<p> For source code, please refer to <a href="https://github.com/booooooooooooooo/sjsuMasterProject/tree/master/classicChinesePoemGenerator"> Link to my Github repository </a></p>
