\documentclass{beamer}

\usetheme[hideothersubsections]{Berkeley}

% these are typical

\usepackage{latexsym}		% to get LASY symbols
\usepackage{epsfig}		% to insert PostScript figures
\usepackage{graphicx}           % to insert any other kind of figure

% these are for math stuff

\usepackage{amsmath}	% AMS math features (e.g., eqn alignment)
\usepackage{amssymb}	% Various weird symbols
\usepackage{amsfonts}	% Various useful math fonts
\usepackage{amsthm}	% Fancy theorem-type environments

% Uncomment these lines to make full-size pages for printing
%\usepackage{pgfpages}
%\pgfpagesuselayout{resize to}[letterpaper,border shrink=5mm,landscape]

% {definition}, {example}, {examples}, {lemma}, {theorem}, and {fact}
% already defined
\newtheorem{question}{Question}
\newtheorem{conjecture}{Conjecture}

\begin{document}

\raggedright

\title{A Question Answering System Using Encoder-decoder Sequence-to-sequence Recurrent Neural Networks}
\author{Bo Li}
\date{May, 2018}
\institute[SJSU]{San Jos\'{e} State University}

\begin{frame}
\titlepage

\end{frame}

\begin{frame}

  \frametitle{Outline}

  \tableofcontents[hideallsubsections]

\end{frame}



\section{Introduction}

\begin{frame} \frametitle{Question Answering}
    \begin{itemize}
        \item the study of writing computer programs that can answer natural language questions
        \item are widely used among search engines, personal assistant applications on smart phones, voice control systems and various other applications
        \item  can be categorized into two types - open domain and close domain
    \end{itemize}
\end{frame}

\begin{frame}{Open Domain \& Close Domain}
    For an open domain system, the questions can be about almost everything; whereas, for a close domain system, the questions are about a specific domain.
\end{frame}

\begin{frame}{Topic of This Project}
    \begin{itemize}
        \item open domain
        \item a scenario where a specific passage is assigned to a question and the answer is a segment of the passage
        \item The Stanford Question Answering Dataset (SQuAD) used in this project is appropriate for such scenario
    \end{itemize}
\end{frame}

\begin{frame}{Stanford Question Answering Dataset (SQuAD)}
    \begin{itemize}
        \item includes questions asked by human beings on Wikipedia articles
        \item The answer to each question is a segment of the corresponding Wikipedia passage
        \item contains more than 100,000 question-answer pairs on more than 500 articles
    \end{itemize}
\end{frame}
\begin{frame}{Encoder-decoder Sequence-to-sequence Recurrent Neural Networks}
   \begin{itemize}
       \item These networks encode an input sequence to some vectors and then decode them to an output sequence. \item For question answering, the input sequence includes a passage and a question and the output sequence is the answer
   \end{itemize}
\end{frame}

\begin{frame}{Contribution of This Project}
    \begin{itemize}
        \item successfully built a question answering system using five different models
        \item By comparing the results of five different models, we got two interesting observations
    \end{itemize}
\end{frame}

\section{Background}

\begin{frame} \frametitle{Word Feature Vector}
  \begin{itemize}
      \item A word feature vector represents a word according to its relationship with other words in a vocabulary.
      \item The distance from one word feature vector to any other word feature vector tells how likely the two words appear in a same context.
      \item The word feature vector matrix for the vocabulary of a given text are learned by training a neural probabilistic language model on the text

  \end{itemize}
\end{frame}

\begin{frame}{Word Feature Vector, cont.}
    neural probabilistic language model
    \begin{itemize}
        \item Denote V as the vocabulary, $w_t$ as a word from $V$, and the matrix $C$ as the word feature vectors of all words in $V$.
        \item Each instance of the training set is a sequence of words $w_1,...,w_T$ which is a segment of the text.
        \item The purpose of a neural probabilistic language model is to learn a model $f$ such that
        $$ f(w_t, ..., w_{t-n+1}) = \hat{P}(w_t | w_{t-1},...,w_{t-n+1}).$$

    \end{itemize}
\end{frame}

\begin{frame}{Word Feature Vector, cont.}
    neural probabilistic language model
    \begin{itemize}
        \item The computation of $f(w_t, ..., w_{t-n+1})$ is divided into two parts:
            \begin{itemize}
                \item First, each $w$ is mapped to a word feature vector by selecting the corresponding row in $C$ to get

                $$x=(C(w_{t-1}),... ,C(w_{t-n+1})).$$

                \item Second, we get $f(w_t, ..., w_{t-n+1})$ through

                $$y=b+W\cdot x + U\cdot tanh(d + H\cdot x)$$

                and

                $$ f(w_t, ..., w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}^{}e^{y_i}}.$$
            \end{itemize}
        \item The loss function to minimize is $$L = -\frac{1}{T}\sum _{t}^{} \log{f(w_t, ..., w_{t-n+1})}.$$
    \end{itemize}
\end{frame}

\begin{frame}{Word Feature Vector, cont.}
   \begin{itemize}
       \item In the neural probabilistic language model, the word feature vectors are used to predict the next word after a sequence. However, the usage of word feature vectors is far beyond this.
       \item Using word feature vectors to represent words is common when applying neural network models on natural language processing tasks. This is how we used word feature vectors in this project.
   \end{itemize}

\end{frame}
\begin{frame}{Recurrent Neural Networks}

Recurrent Neural Networks (RNNs) are used for modeling sequential data

    \begin{examples}{A simple recurrent network with no outputs}
        \begin{itemize}
            \item $x$ is the input. $h$ is the state. $\theta$ is the hyperparameter.
            \item The relation between $h$ and $x$ is
            $$h_t = f(h_{t-1}, x_t; \theta).$$

            \item An example of $f$ is

            $$h_t = sigmoid(W_h h_{t-1} + W_x x_t + b).$$
        \end{itemize}
    \end{examples}

\end{frame}

\begin{frame}{Recurrent Neural Networks, cont.}
    \begin{center}
      \includegraphics[width=9cm, height=3cm]{figures/rnnWithNoOutputs}
    \end{center}
\end{frame}

\begin{frame}{Recurrent Neural Networks, cont.}
    \begin{itemize}
        \item the vanishing gradient problem exists
            \begin{itemize}
                \item the vanishing gradient means the gradients become smaller and smaller as their values are propagated forward in a network
                \item When this happens, the network learns slowly or even stops learning
            \end{itemize}
        \item The main solution to the vanishing problem is to use a more complex learning unit
        \item Long Short Term Memory (LSTM) cell is one such complex learning unit; Gated Recurrent Unit (GRU) has a simplified structure but similar function with LSTM
        \item In this project, I used LSTM and GRU equally as learning unit
    \end{itemize}
\end{frame}

\begin{frame}{Bidirectional Recurrent Neural Networks}
    \begin{itemize}
        \item Problem of RNNs: $h_t$ only contains context information from $x_1$ to $x_t$
        \item Solution given by Bidirectional RNNs:
        \begin{itemize}
            \item one cell operates from left to right, and another cell operates from right to left
            \item using both $h_t$ and $g_t$ can get context information of the whole sequence
        \end{itemize}
    \end{itemize}

    \begin{center}
        \includegraphics[width=8cm, height=5cm]{figures/bidirectionalRnn.png}
    \end{center}

\end{frame}

\begin{frame}{Encoder-decoder Sequence-to-sequence Architecture}
\begin{itemize}
    \item The process of understanding the input sequence is considered as encoding the input sequence to some vectors $Crypto$.
    \item The process of generating output is considered as decoding the $Crypto$.
\end{itemize}

\begin{examples}
$x$ is the input, $h$ is the state in encoding process, $y$ is the output, and $g$ is the state of decoding process.
\end{examples}

\end{frame}

\begin{frame}{Encoder-decoder Sequence-to-sequence Architecture, Cont.}
    \begin{center}
        \includegraphics[width=10cm, height=7cm]{figures/encoderDecoder.png}
    \end{center}
\end{frame}

\begin{frame}{Encoder-decoder Sequence-to-sequence Architecture, Cont.}
    \begin{itemize}
        \item The question answering task in this project is a sequence-to-sequence task.
        \item Each input actually includes two sequences - a question and a passage.
            \begin{itemize}
                \item attention mechanism is required to make each passage aware of the corresponding question and encode the two together
            \end{itemize}
        \item each output sequence is an answer which is represented by two indices for the input passage sequence.
            \begin{itemize}
                \item pointer network is required to make sure the output sequence comes from input sequence
            \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}{Attention Mechanism}
    \begin{itemize}
        \item first used in neural machine translation
        \item is used to enable the decoding process aware of the encoding states
    \end{itemize}
\end{frame}

\begin{frame}{Attention Mechanism, Cont.}
    \begin{examples}
        $y$ is the output, $g$ is the state, and $c$ is the attention vector. $$g_i =f(g_{i-1},y_{i-1},c_i).$$
        The attention vector $c_i$ is produced by using $g_{i-1}$ to ``query'' the encoding states $h_1, ... h_n$ through
        $$c_i = \sum _j {\alpha _{i,j} h_j}$$
        $$\alpha _{i,j} = \exp{e_{i,j}} / \sum _j {\exp{e_{i,j}}}$$
        $$e_{i,j} = tanh(W_h h_j + W_g g_{i-1} + b).$$
    \end{examples}

\end{frame}

\begin{frame}{Attention Mechanism, Cont.}
    \begin{center}
        \includegraphics[width=8cm, height=8cm]{figures/attention}
    \end{center}
\end{frame}

\begin{frame}{Attention Mechanism, Cont.}
    In this project, the passage is required to ``be aware of the question'' in encoding process. At the same time, the answer is required to ``be aware of the encoding states of passage and question''. The detailed formulas are given later.
\end{frame}

\begin{frame}{Pointer Network}
    \begin{itemize}
        \item Using the pointer network enables the decoder to output tokens from input sequence
        \item The attention mechanism is used with the pointer network
        \item However, aside from getting an attention vector, the attention weight vector $\alpha$ is considered as a probability distribution which indicates how likely each token in the input sequence is the current output.
        $$y_i = x_k$$
        where
        $$k = argmax_j(\alpha _{i,j}).$$
    \end{itemize}
\end{frame}

\begin{frame}{Pointer Network, Cont.}
    \begin{center}
        \includegraphics[width=8cm, height=8cm]{figures/pointerNetwork.png}
    \end{center}

\end{frame}

\begin{frame}{Pointer Network, Cont.}
    In this project, the pointer network was used in the decoding part of several models.
\end{frame}
\section{Design}

\begin{frame}{}
    In this chapter, I will explain the five models one by one. Recall that Model 1 is the Match LSTM \& Answer Pointer model which has a typical encoder-decoder sequence-to-sequence recurrent network architecture designed by Wang and Jiang. Model 2, 3, 4 and 5 are designed by making changes to Model 1.
\end{frame}

\begin{frame} \frametitle{Model 1}
  \begin{itemize}
      \item an encoder-decoder sequence-to-sequence architecture
      \item on SQuAD dataset.Each instance of training data includes one passage, one question and one answer. The passage is a sequence of tokens, the question is a sequence of tokens, and the answer is a sequence of two indices indicating the start and end positions in passage.
      \item Before feeding training data into the model, tokens in passages and questions are vectorized to word feature vectors.
  \end{itemize}
\end{frame}

\begin{frame} \frametitle{Model 1, cont.}\framesubtitle{Structure Overview}
    \begin{itemize}
        \item Encoder
            \begin{itemize}
                \item the preprocessing layer
                \item the bidirectional match LSTM layer
            \end{itemize}
        \item Decoder
            \begin{itemize}
                \item the answer pointer layer
            \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame} \frametitle{Model 1, cont.}\framesubtitle{the preprocessing layer}

    \begin{itemize}
        \item a LSTM runs over each passage word feature vector sequence and outputs a sequence of LSTM states
        \item The same LSTM is used to encode each question word feature vector sequence to a sequence of LSTM states.
    \end{itemize}
\end{frame}

\begin{frame} \frametitle{Model 1, cont.}\framesubtitle{the preprocessing layer}
    $$H^p = \overrightarrow{LSTM}(P)$$
    $$H^q = \overrightarrow{LSTM}(Q)$$

    where

    $$P\in R^{d \times p}: passage$$
    $$Q\in R^{d \times q}: question$$
    $$H^p\in R^{l \times p}: encoded\ passage$$
    $$H^q\in R^{l \times q}: encoded\ question$$
    $$p: length \ of\ passage$$
    $$q: length\ of\ question$$
    $$l: dimension\ of\ LSTM\ states$$
    $$d: dimension\ of\ word\ feature\ vector$$

\end{frame}

\begin{frame} \frametitle{Model 1, cont.}\framesubtitle{bidirectional match LSTM layer}

a LSTM equipped with passage-question attention, which is called match LSTM, is used to encode each sequence of passage states and the corresponding sequence of question states together to a sequence of match LSTM states.


\end{frame}

\begin{frame} \frametitle{Model 1, cont.}\framesubtitle{bidirectional match LSTM layer}

    $$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + W^r\overrightarrow{{h_{i-1}}^r} + b^p) \otimes e_q)$$
    $$\overrightarrow{\alpha _i} = softmax(w^t\overrightarrow{G_i} + b \otimes e_q)$$


    where

    $$W^q, W^p, W^r\in R^{l \times l} $$
    $$b_p, w\in R^{l}  $$
    $$b \in R $$
    $${h_{i}^p}\in R^{l}: one\ column\ of\ H^p  $$

    and

    \[ \overrightarrow{z_i} =
    \begin{bmatrix}
    {h_i}^p \\
    H^q\overrightarrow{ {\alpha _i}}^T \\
    \end{bmatrix}
    \in R^{2l}
    \]
    $$\overrightarrow{{h_i}^r} = \overrightarrow{LSTM}(\overrightarrow{z_i}, \overrightarrow{{h_{i-1}}^r}).$$

\end{frame}

\begin{frame} \frametitle{Model 1, cont.}\framesubtitle{bidirectional match LSTM layer}
    After iterating between getting attention weight vector $\overrightarrow{\alpha _i}$ and getting match LSTM state ${{h_{i}}^r}$ $p$ times, we get $[{{h_{1}}^r}, ..., {{h_{p}}^r}]$. Concatenate them to get

    $$\overrightarrow{H^r} = [{{h_{1}}^r}, ..., {{h_{p}}^r}] \in R^{l \times p}.$$

    Then go over $H^p$ from right to left to get $\overleftarrow{H^r}$. Concatenate $\overrightarrow{H^r}$ and $\overleftarrow{H^r}$ to get the final output of encoding process

    \[ H^r =
    \begin{bmatrix}
    \overrightarrow{H^r} \\
    \overleftarrow{H^r} \\
    \end{bmatrix}
    \in R^{2l \times p}.
    \]
\end{frame}

\begin{frame} \frametitle{Model 1, cont.}\framesubtitle{answer pointer layer}
each output of the decoding process includes two probability distributions.
   \begin{itemize}
       \item The first probability distribution tells how likely each token in passage to be the start of the answer.
       \item The second probability distribution tells how likely each token in passage to be the end of the answer
   \end{itemize}
\end{frame}

\begin{frame}\frametitle{Model 1, cont.}\framesubtitle{answer pointer layer}
    $$F_k = tahn(VH^r + (W^a{h^a_{k-1}} +  b^a) \otimes e_p)$$
    $$\beta _k = softmax(v^tF_k + c \otimes e_p)$$


    where
    $$V \in R^{l \times 2l}$$
    $$W^a\in R^{l \times l} $$
    $$b_a, v\in R^{l}  $$
    $$c \in R $$
    $${h_{k-1}}^a\in R^{l}: ith\ state\ of\ answer\ LSTM  $$

    and answer LSTM is


    $${h_k}^a = LSTM(H^r\beta _k^T, h_{k-1}^a)$$
\end{frame}

\begin{frame}\frametitle{Model 1, cont.}\framesubtitle{answer pointer layer}
    By iterating between the attention mechanism and the answer LSTM two times, we could get the output of the decoding process - $\beta _0$ and $\beta _1$.


    Now we can get the loss function. Let $a_s$ denote the ground truth start index of the answer and $a_e$ denote the ground truth end index, we have

    $$p(a|H^r) = p(a_s|H_r)p(a_r|H_r)=\beta _{0, a_s} \times \beta_{1, a_e}$$

    where $$\beta_{k, j} = jth\ token\ of\ \beta _k$$

\end{frame}

\begin{frame}\frametitle{Model 1, cont.}\framesubtitle{loss function}
    To train the model, the loss function

    $$J(\theta) = -\frac{1}{N}\sum_{i=1}^{N} \log{p(a^n|H^r)} $$

    is minimized.
\end{frame}

\begin{frame} \frametitle{Model 2}
    The difference from Model 2 and Model 1 is in the decoding process. In Model 2,
    $${h_k}^a = H^r\beta _{k}^T.$$
    That is, instead of the current state of answer LSTM, the previous attention vector is used to query the current attention weight vector.
\end{frame}

\begin{frame} \frametitle{Model 3}
    The difference between Model 3 and Model 2 is that in Model 3 the $W^r\overrightarrow{{h_{i-1}}^r}$ in the bidirectional match LSTM layer is removed. This modification aims at checking whether $\overrightarrow{{h_{i-1}}^r}$ carries some redundant context information. After this change,


    $$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + b^p) \otimes e_q)$$
\end{frame}

\begin{frame} \frametitle{Model 4}
    The difference between Model 4 and Model 2 is that in Model 4 the the preprocessing layer is removed. This modification aims at checking whether the preprocessing layer carries some redundant context information.
\end{frame}

\begin{frame} \frametitle{Model 5}
    The difference between Model 5 and Model 2 is that in Model 5 both the preprocessing layer and $W^r\overrightarrow{{h_{i-1}}^r}$ in the bidirectional match LSTM layer are removed. This aims at checking whether context information carried by both is included in some other parts of Model 2.
\end{frame}

\section{Implementation}

\begin{frame} \frametitle{Adjusting Models for Batch Training}

\end{frame}


\begin{frame}{Tensorflow Graphs}

\end{frame}

\begin{frame}{Implementation Pipeline}

\end{frame}

\section{Experiments}

\begin{frame} \frametitle{Data}

\end{frame}

\begin{frame}{Settings}

\end{frame}

\begin{frame}{Training Process}

\end{frame}

\begin{frame}{Testing Results}

\end{frame}

\begin{frame}{Analysis}

\end{frame}

\section{Conclusion}

\begin{frame}{Contribution of This Project}

\end{frame}

\begin{frame} \frametitle{Future Work}

\end{frame}

\begin{frame}
Thank you! Questions?
\end{frame}

\end{document}
