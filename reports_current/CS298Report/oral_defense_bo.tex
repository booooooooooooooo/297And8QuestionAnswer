\documentclass{beamer}

\usetheme[hideothersubsections]{Berkeley}

% these are typical

\usepackage{latexsym}		% to get LASY symbols
\usepackage{epsfig}		% to insert PostScript figures
\usepackage{graphicx}           % to insert any other kind of figure

% these are for math stuff

\usepackage{amsmath}	% AMS math features (e.g., eqn alignment)
\usepackage{amssymb}	% Various weird symbols
\usepackage{amsfonts}	% Various useful math fonts
\usepackage{amsthm}	% Fancy theorem-type environments

% Uncomment these lines to make full-size pages for printing
%\usepackage{pgfpages}
%\pgfpagesuselayout{resize to}[letterpaper,border shrink=5mm,landscape]

% {definition}, {example}, {examples}, {lemma}, {theorem}, and {fact}
% already defined
\newtheorem{question}{Question}
\newtheorem{conjecture}{Conjecture}

\begin{document}

\raggedright

\title{A Question Answering System Using Encoder-decoder Sequence-to-sequence Recurrent Neural Networks}
\author{Bo Li}
\date{May, 2018}
\institute[SJSU]{San Jos\'{e} State University}

\begin{frame}
\titlepage

\end{frame}

\begin{frame}

  \frametitle{Outline}

  \tableofcontents[hideallsubsections]

\end{frame}



\section{Introduction}

\begin{frame} \frametitle{Question Answering}
    \begin{itemize}
        \item the study of writing computer programs that can answer natural language questions
        \item are widely used among search engines, personal assistant applications on smart phones, voice control systems and various other applications
        \item  can be categorized into two types - open domain and close domain
    \end{itemize}
\end{frame}

\begin{frame}{Open Domain \& Close Domain}
    For an open domain system, the questions can be about almost everything; whereas, for a close domain system, the questions are about a specific domain.
\end{frame}

\begin{frame}{Topic of This Project}
    \begin{itemize}
        \item open domain
        \item a scenario where a specific passage is assigned to a question and the answer is a segment of the passage
        \item The Stanford Question Answering Dataset (SQuAD) used in this project is appropriate for such scenario
    \end{itemize}
\end{frame}

\begin{frame}{Stanford Question Answering Dataset (SQuAD)}
    \begin{itemize}
        \item includes questions asked by human beings on Wikipedia articles
        \item The answer to each question is a segment of the corresponding Wikipedia passage
        \item contains more than 100,000 question-answer pairs on more than 500 articles
    \end{itemize}
\end{frame}
\begin{frame}{Encoder-decoder Sequence-to-sequence Recurrent Neural Networks}
   \begin{itemize}
       \item These networks encode an input sequence to some vectors and then decode them to an output sequence. \item For question answering, the input sequence includes a passage and a question and the output sequence is the answer
   \end{itemize}
\end{frame}

\begin{frame}{Contribution of This Project}
    \begin{itemize}
        \item successfully built a question answering system using five different models
        \item By comparing the results of five different models, we got two interesting observations
    \end{itemize}
\end{frame}

\section{Background}

\begin{frame} \frametitle{Word Feature Vector}
  \begin{itemize}
      \item A word feature vector represents a word according to its relationship with other words in a vocabulary.
      \item The distance from one word feature vector to any other word feature vector tells how likely the two words appear in a same context.
      \item The word feature vector matrix for the vocabulary of a given text are learned by training a neural probabilistic language model on the text

  \end{itemize}
\end{frame}

\begin{frame}{Word Feature Vector, cont.}
    neural probabilistic language model
    \begin{itemize}
        \item Denote V as the vocabulary, $w_t$ as a word from $V$, and the matrix $C$ as the word feature vectors of all words in $V$.
        \item Each instance of the training set is a sequence of words $w_1,...,w_T$ which is a segment of the text.
        \item The purpose of a neural probabilistic language model is to learn a model $f$ such that
        $$ f(w_t, ..., w_{t-n+1}) = \hat{P}(w_t | w_{t-1},...,w_{t-n+1}).$$

    \end{itemize}
\end{frame}

\begin{frame}{Word Feature Vector, cont.}
    neural probabilistic language model
    \begin{itemize}
        \item The computation of $f(w_t, ..., w_{t-n+1})$ is divided into two parts:
            \begin{itemize}
                \item First, each $w$ is mapped to a word feature vector by selecting the corresponding row in $C$ to get

                $$x=(C(w_{t-1}),... ,C(w_{t-n+1})).$$

                \item Second, we get $f(w_t, ..., w_{t-n+1})$ through

                $$y=b+W\cdot x + U\cdot tanh(d + H\cdot x)$$

                and

                $$ f(w_t, ..., w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}^{}e^{y_i}}.$$
            \end{itemize}
        \item The loss function to minimize is $$L = -\frac{1}{T}\sum _{t}^{} \log{f(w_t, ..., w_{t-n+1})}.$$
    \end{itemize}
\end{frame}

\begin{frame}{Word Feature Vector, cont.}
   \begin{itemize}
       \item In the neural probabilistic language model, the word feature vectors are used to predict the next word after a sequence. However, the usage of word feature vectors is far beyond this.
       \item Using word feature vectors to represent words is common when applying neural network models on natural language processing tasks. This is how we used word feature vectors in this project.
   \end{itemize}

\end{frame}
\begin{frame}{Recurrent Neural Networks}

Recurrent Neural Networks (RNNs) are used for modeling sequential data

    \begin{examples}{A simple recurrent network with no outputs}
        \begin{itemize}
            \item $x$ is the input. $h$ is the state. $\theta$ is the hyperparameter.
            \item The relation between $h$ and $x$ is
            $$h_t = f(h_{t-1}, x_t; \theta).$$

            \item An example of $f$ is

            $$h_t = sigmoid(W_h h_{t-1} + W_x x_t + b).$$
        \end{itemize}
    \end{examples}

\end{frame}

\begin{frame}{Recurrent Neural Networks, cont.}
    \begin{center}
      \includegraphics[width=9cm, height=3cm]{figures/rnnWithNoOutputs}
    \end{center}
\end{frame}

\begin{frame}{Recurrent Neural Networks, cont.}
    \begin{itemize}
        \item the vanishing gradient problem exists
            \begin{itemize}
                \item the vanishing gradient means the gradients become smaller and smaller as their values are propagated forward in a network
                \item When this happens, the network learns slowly or even stops learning
            \end{itemize}
        \item The main solution to the vanishing problem is to use a more complex learning unit
        \item Long Short Term Memory (LSTM) cell is one such complex learning unit; Gated Recurrent Unit (GRU) has a simplified structure but similar function with LSTM
        \item In this project, I used LSTM and GRU equally as learning unit
    \end{itemize}
\end{frame}

\begin{frame}{Bidirectional Recurrent Neural Networks}
    \begin{itemize}
        \item Problem of RNNs: $h_t$ only contains context information from $x_1$ to $x_t$
        \item Solution given by Bidirectional RNNs:
        \begin{itemize}
            \item one cell operates from left to right, and another cell operates from right to left
            \item using both $h_t$ and $g_t$ can get context information of the whole sequence
        \end{itemize}
    \end{itemize}

    \begin{center}
        \includegraphics[width=8cm, height=5cm]{figures/bidirectionalRnn.png}
    \end{center}

\end{frame}

\begin{frame}{Encoder-decoder Sequence-to-sequence Architecture}

\end{frame}

\begin{frame}{Attention Mechanism}

\end{frame}

\begin{frame}{Pointer Network}

\end{frame}

\section{Design}

\begin{frame} \frametitle{Model 1}

\end{frame}

\begin{frame} \frametitle{Model 2}

\end{frame}

\begin{frame} \frametitle{Model 3}

\end{frame}

\begin{frame} \frametitle{Model 4}

\end{frame}

\begin{frame} \frametitle{Model 5}

\end{frame}

\section{Implementation}

\begin{frame} \frametitle{Adjusting Models for Batch Training}

\end{frame}


\begin{frame}{Tensorflow Graphs}

\end{frame}

\begin{frame}{Implementation Pipeline}

\end{frame}

\section{Experiments}

\begin{frame} \frametitle{Data}

\end{frame}

\begin{frame}{Settings}

\end{frame}

\begin{frame}{Training Process}

\end{frame}

\begin{frame}{Testing Results}

\end{frame}

\begin{frame}{Analysis}

\end{frame}

\section{Conclusion}

\begin{frame}{Contribution of This Project}

\end{frame}

\begin{frame} \frametitle{Future Work}

\end{frame}

\begin{frame}
Thank you! Questions?
\end{frame}

\end{document}
