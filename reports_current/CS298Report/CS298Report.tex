%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  Example usage of sjsuthesis.cls %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[modernstyle,12pt]{sjsuthesis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%    load any packages which are needed    %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% these are typical

\usepackage{latexsym}		% to get LASY symbols
\usepackage{epsfig}		% to insert PostScript figures
\usepackage{graphicx}           % to insert any other kind of figure

% these are for math stuff

\usepackage{amsmath}	% AMS math features (e.g., eqn alignment)
\usepackage{amssymb}	% Various weird symbols
\usepackage{amsfonts}	% Various useful math fonts
\usepackage{amsthm}	% Fancy theorem-type environments

% Convention: everything (except pictures) is numbered inside a single
% sequence, starting again in each section.  This makes things much
% easier to read.

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem*{main}{Main Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exmp}[thm]{Example}
\newtheorem{ques}[thm]{Question}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%       all the preamble material:       %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{A Question Answering System Using Encoder-Decoder Sequence-to-sequence Recurrent Neural Networks}

\author{BO}{LI}

\degree{Master of Science}		%  #1 {long descr.}
	{M.S., Computer Science}		%  #2 {short descr.}

\degreemonth{May}
\degreeyear{2018}

\dept{Department of}			%  #1 {designation}
     {Computer Science}		        %  #2 {name}

\advisor{Dr.}				%  #1 {title}
	{Chris Pollett}			%  #2 {name}
\advisorOrg{Department of Computer Science}

\reader{Dr.~Suneuy Kim}		        %  2nd person to sign thesis
\readerOrg{Department of Computer Science}

\readerThree{Dr.~David Taylor}		%  3rd person to sign thesis
\readerThreeOrg{Department of Computer Science}

% you can optionally add \readerFour and \readerFive as well

%\readerFour{Dr.~Who Dat}		%  4th person to sign thesis
%\readerFourOrg{Department of Physics, Harvard Univ.}

% NOTE: to get the front matter single spaced, put \singlespacing
% at the start of the abstract text

\abstract{Question Answering is about making computer programs that can answer natural language questions. It is one of the most challenging tasks in natural language processing. At the present, the state-of-art performance of Question Answering is produced by applying neural network models. In this project, I successfully built a Question Answering system using encoder-decode sequence-to-sequence recurrent neural networks. In total, I tried five different models including a baseline model called Match-lstm and Answer Pointer model and four experiments based on it. Through experiments, I got two interesting observations. First, the testing performance increases when querying attention using weighted encoding states instead of decoding state in Answer Pointer layer. Second, Preprocessing layer and the $h_r$ in Bidirectional Match-LSTM layer might provide duplicate context information.
}



% acknowledgements page is optional

\acknowledgements{

Thanks to my advisor Dr. Pollett for his patience, support and guidance during this project. Thanks to Dr. Kim and Dr. Taylor for their generous service in committee.
}

% the following options can be enabled or disabled

%\ToCisShort	% a 1-page Table of Contents

% Default: List of figures will be printed
% Uncomment the \emptyLoF line to skip the list of figures
%\LoFisShort	% a 1-page List of Figures
%\emptyLoF	% no List of Figures at all

%\LoTisShort	% a 1-page List of Tables
% \emptyLoT	% no List of Tables at all


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%       BEGIN DOCUMENT...         %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% the \begin{document} will generate all the prologue material
% (signature page, TOC, etc.); if you want to control this
% behavior, uncomment one of the following lines:
%
% \SuspendPrologue    % disables the prologue entirely
% \SimplePrologue     % prints only title, abstract, TOC, TOF


% the following command will cause a draft version and the
% current date to be printed in the header area
%
% \draftVersion{1}


\begin{document}

\raggedright          % as per SJSU GS&R guidelines June 2010
\parindent=30pt       % restores indentation

% \singlespacing      % uncomment to print single spaced (e.g., for drafts)


% document body goes here

\chapter{Introduction}

Question Answering (QA) is about making computer programs that can answer natural language questions. QA techniques are widely used among search engines, personal assistant applications on smart phones, voice control systems and a lot more other applications. QA is one of the most challenging tasks in natural language processing. In recent years, more neural network models have been built to do natural language processing tasks. This approach gives more accurate result than traditional solutions, which use syntactic and semantic analysis and hand made features. Among various neural network architectures, encoder-decoder sequence-to-sequence recurrent neural networks are quite suitable for many natural language processing tasks. Such networks encode an input sequence to some vectors and then decode them to an output sequence. Since QA is also a sequence-to-sequence task, researching on how to apply the encoder-decoder sequence-to-sequence recurrent neural networks to do QA is meaningful.


In this project, I successfully built a QA system using five different models. The baseline model is the Match-lstm and Answer pointer model which is designed by Wang and Jiang\cite{wang2016machine}. The model has a typical encoder-decode sequence-to-sequence recurrent network architecture and has a network size which is not too big to train using the limited computation resource I can access. Based on the baseline model, I designed four experiments to understand it and try to improve it. Through comparing the results of baseline model and 4 experiments, I got two interesting observations. First, the performance improves when querying attention using a weighted average of encoding states instead of a decoding lstm state in Answer Pointer layer. Second, Preprocessing layer and the $h_r$ in Bidirecional Match-LSTM layer might provide duplicate context information.


The dataset used in this project is the Stanford Question Answering Dataset (SQuAD). As described by \cite{rajpurkar2016squad}, SQuAD includes questions asked by human beings on Wikipedia articles. The answer to each question is a segment of the corresponding Wikipedia article. In total, SQuAD contains 100,000+ question-answer pairs on 500+ articles.




\chapter{Background}\label{chap:background}
\section{Word Feature Vector}

Word Feature Vector (WFV) was firstly came up with by Bengio, Yoshua and Ducharme in \cite{bengio2003neural}. A word feature vector represents a word according to its relationship with other words in the vocabulary. The distance from a word feature vector to any other word feature vector tells how likely the two words appear in a same context.

The word feature vectors of the vocabulary from a given text are learned from training a neural probabilistic language model (NPLM) on the text. Denote V as the vocabulary, $w_t$ as a word from $V$, and the matrix $C$ as the word feature vectors of all words in $V$. Each instance of the training set is a sequence of words $w_1,...,w_T$ which is a segment of the text. The purpose of NPLM is to train a model $f$ such that

$$ f(w_t, ..., w_{t-n+1}) = \hat{P}(w_t | w_{t-1},...,w_{t-n+1}).$$

The computation of $f(w_t, ..., w_{t-n+1})$ is divided into two parts.
First, each $w$ is mapped to a WFV by selecting the corresponding row in $C$ to get

$$x=(C(w_{t-1}),... ,C(w_{t-n+1})).$$

Second, we get $f(w_t, ..., w_{t-n+1})$ through

$$y=b+W\cdot x + U\cdot tanh(d + H\cdot x)$$

and

$$ f(w_t, ..., w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}^{}e^{y_i}}.$$

The loss function to minimize is $$L = -\frac{1}{T}\sum _{t}^{} \log{f(w_t, ..., w_{t-n+1})}.$$


WFV enables learning dependencies on longer sentences using reasonable computation resource and time. In comparison, although n-grams could learn long term dependency by choosing large n, in practice, current computation resource could only support $n$ to be around 3. As such, a natural language model built on WFV can consider more context information than that built on n-grams. More context generally brings more accurate results.

The usage of WFV is far beyond simply predicting a word's neighbours. In practice, WFV is a common way to represent the words in a neural network model which does a natural language processing task.

In this project, I used WFV to initialize passage and question tokens.

\section{Recurrent Neural Networks (RNNs)}\label{sect:rnn}

Recurrent neural networks (RNNs) \cite{rumelhart1986learning} are used for modeling sequential data. Figure \ref{f:rnnWithNoOutputs} shows a simple recurrent network with no outputs. $x$ is the input. $h$ is the state. $\theta$ is the hyperparameter. The relation between $h$ and $x$ is

$$h_t = f(h_{t-1}, x_t; \theta).$$

An example of $f$ is

$$h_t = sigmoid(W_h h_{t-1} + W_x x_t + b).$$

\begin{figure}[htbp]\centering
  \includegraphics[width=9cm, height=3cm]{figures/rnnWithNoOutputs}
  \caption{A simple recurrent network}
  \label{f:rnnWithNoOutputs}
\end{figure}



Despite the fitness of applying RNNs to sequential data, vanishing problem exists. Vanishing means the gradients become smaller and smaller as the network going forward. When this happens, the networks are learning very slow or even stop learning. The main solution to vanishing problem is using a more complex learning unit. In 1997, hochreiter invented Long Short Term Momory (LSTM) cell \cite{hochreiter1997long} which decreases the vanishing problem. LSTM has one more memory cell to remember long term context and use forget gate, input gate and output gate to control how much information to flow into and out of the current unit. Aside from LSTM, Cho et al. invented Gated Recurrent Unit (GRU)\cite{cho2014learning} which has a simplified structure but similar function with LSTM.


In this project, I used LSTM and GRU equally as learning unit. Among various RNN structures, I mainly use two types. The first type is a RNN with recurrent connections between hidden states as Figure \ref{f:rnnWithNoOutputs}. The sequence of  states are needed. The second type is also a RNN with recurrent connections between hidden states. However, the last state is needed.

\section{Bidirectional RNNs}

The RNNs of Section \ref{sect:rnn}
iterate from left to right. As such, the $h_t$ only contains context information from $x_1$ to $x_t$, but does not contain context information from $x_{t+1}$ to the end. However, in most sequence-to-sequence tasks, $h_t$ should contain the information of the whole sequence. Bidirectional RNNs make this possible. In a bidirectional RNN, one cell rolls from left to right, and another cell rolls from right to left. As illustrated in Figure \ref{f:bidirectionalRnn}, at time t, using both $h_t$ and $g_t$ can get context information of the whole sequence.

\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=5cm]{figures/bidirectionalRnn.png}
  \caption{A simple bidirectional recurrent neural network}
  \label{f:bidirectionalRnn}
\end{figure}

In this project, I used bidirectional RNNs in encoding part.

\section{Encoder-Decoder Sequence-to-Sequence Architecture}

Sequence-to-sequence means the input to the model is a sequence and the output from the model is also a sequence. An encoder-decoder architecture can be applied to do this task. The process of understanding the input sequence is considered as encoding the input sequence to some vectors $Crypto$. The process of generating output is considered as decoding the $Crypto$. Figure \ref{f:encoderDecoder} shows the concept of encoder-decoder sequence-to-sequence architecture . $x$ is the input, $h$ is the state in encoding process, $y$ is the output, and $g$ is the state of decoding process.

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/encoderDecoder.png}
  \caption{The concept of encoder-decoder sequence-to-sequence architecture}
  \label{f:encoderDecoder}
\end{figure}

The question answering task in this project is a sequence-to-sequence task. However, some additional techniques must be equipped to the basic architecture in Figure \ref{f:encoderDecoder}. Each input actually includes two sequences - a question and a passage. As such, in the encoding process, some method is required to make each passage aware of the corresponding question and encode them together. The attention mechanism discussed in Section \ref{sect:attention} is one such method. At the same time, each output sequence is an answer which is represented by two indices of the input passage sequence. A special decoding technique called Pointer Network discussed in Section \ref{sect:pointerNet} is needed.

\section{Attention Mechanism}\label{sect:attention}

Attention mechanism was firstly came up with by Bahdanau et al. \cite{bahdanau2014neural} in the application of neural machine translation. In the neural machine translation task, an encoder-decoder sequence-to-sequence model encodes each input sentence to some vectors and decodes the vectors to a sentence in another language with the same content. The attention mechanism was used to enable the decoding process know about the encoding states $h$. As shown in Figure \ref{f:attention}, $y$ is output, $g$ is state, and $c$ is attention vector. We have
$$g_i =f(g_{i-1},y_{i-1},c_i).$$
The attention vector $c_i$ is produced by using $g_{i-1}$ to ``query'' the encoding states $h_1, ... h_n$ through
$$c_i = \sum _j {\alpha _{i,j} h_j}$$
$$\alpha _{i,j} = \exp{e_{i,j}} / \sum _j {\exp{e_{i,j}}}$$
$$e_{i,j} = attention(h_j, g_{i-1}).$$
An example of the $attention$ function is $e_{i,j} = tanh(W_h h_j + W_g g_{i-1} + b)$.


\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=8cm]{figures/attention}
  \caption{Attention mechanism in machine translation}
  \label{f:attention}
\end{figure}



Indeed, attention mechanism is a way to ``be aware of a sequence''. Since being aware of more context is a basic need of natural language processing tasks, it is reasonable to use attention mechanism in other tasks besides machine translation.

In this project, the passage is allowed to ``be aware of the question'' in encoding process. The detailed formulas is given in Chapter \ref{chap:design}. At the same time, the answer is allowed to ``be aware of the encoding states of passage and question''. The detailed formulas is also given in Chapter \ref{chap:design}.




\section{Pointer Network}\label{sect:pointerNet}

Pointer Network\cite{vinyals2015pointer} was invented by Vinyals et al. in 2015. Pointer Network enables decoder to output tokens from input sequence. Attention mechanism is used in pointer network. However, aside from getting an attention vector, the attention weight $\alpha$ is considered as a probability distribution which indicates how likely each token in input sequence is the current output. That is,
$$y_i = x_k$$
where
$$k = argmax_j(\alpha _{i,j}).$$
Note that compared with the machine translation architecture \ref{f:attention}, in pointer network, $y_i$ is not fed into the next decoding state.


\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=8cm]{figures/pointerNetwork.png}
  \caption{The concept of pointer network}
  \label{f:pointerNetwork}
\end{figure}

In this project, the decoding part of model used Pointer Network.


\chapter{Design}\label{chap:design}

In this chapter, I will review the baseline model - Match-LSTM and Answer Pointer model and four experiments based on the baseline model. The baseline model is quite suitable for this project. First, it reflects a general encoder-decoder sequence-to-sequence architecture to do question answering tasks. Second, its network size is not too large to train given the limited computation resource I can access.


\section{Baseline Model: Match-LSTM and Answer Pointer (MLAP) }

Wang and Jiang  proposed an encoder-decoder sequence-to-sequence architecture for the question answering task on SQuAD dataset \cite{wang2016machine}. Each instance of training data includes one passage, one question and one answer. The passage is a sequence of tokens, the question is a sequence of tokens, and the answer is two indices indicating the start and end positions in passage. Recall that each answer is part of the corresponding passage in SQuAD.

Before feeding training data into model, tokens in passages and questions are vectorized to word feature vectors. As such, some pre-trained word feature vector matrix is an additional dataset in need.

The vectorized training data is fed into the encoder. The encoder includes two layers - Preprocessing layer and Bi-directional Match-LSTM layer. In Preprossing layer, a LSTM network runs over each passage word feature vector sequence and outputs a sequence of LSTM states. The same LSTM is used to encode each question word vector sequence to a sequence of LSTM states.

$$H^p = \overrightarrow{LSTM}(P)$$
$$H^q = \overrightarrow{LSTM}(Q)$$

where

 $$P\in R^{d \times p}: passage$$
 $$Q\in R^{d \times q}: question$$
 $$H^p\in R^{l \times p}: encoded\ passage$$
 $$H^q\in R^{l \times q}: encoded\ question$$
 $$p: length \ of\ passage$$
 $$q: length\ of\ question$$
 $$l: dimension\ of\ LSTM\ states$$
 $$d: dimension\ of\ word\ feature\ vector$$

In Bi-directional Match-LSTM layer, a LSTM equipped with passage-question attention, which is called Match-LSTM, is used to encode each sequence of passage states and the corresponding sequence of question states together to a sequence of Match-LSTM states. To be specific,

$$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + W^r\overrightarrow{{h_{i-1}}^r} + b^p) \otimes e_q)$$
$$\overrightarrow{\alpha _i} = softmax(w^t\overrightarrow{G_i} + b \otimes e_q)$$


where

$$W^q, W^p, W^r\in R^{l \times l} $$
$$b_p, w\in R^{l}  $$
$$b \in R $$
$${{h_{i}}pr}\in R^{l}: one\ column\ of\ H^p  $$

and

\[ \overrightarrow{z_i} =
\begin{bmatrix}
{h_i}^p \\
H^q\overrightarrow{ {\alpha _i}}^T \\
\end{bmatrix}
\in R^{2l}
\]
$$\overrightarrow{{h_i}^r} = \overrightarrow{LSTM}(\overrightarrow{z_i}, \overrightarrow{{h_{i-1}}^r}).$$

After iterating between getting attention vector $\overrightarrow{\alpha _i}$ and getting Match-LSTM state ${{h_{i}}^r}$ $p$ times, we get $[{{h_{1}}^r}, ..., {{h_{p}}^r}]$. Concatenate them to get

$$\overrightarrow{H^r} = [{{h_{1}}^r}, ..., {{h_{p}}^r}] \in R^{l \times p}.$$

Then go over $H^p$ from right to left to get $\overleftarrow{H^r}$. Concatenate $\overrightarrow{H^r}$ and $\overleftarrow{H^r}$ to get the final output of encoding process

\[ H^r =
\begin{bmatrix}
\overrightarrow{H^r} \\
\overleftarrow{H^r} \\
\end{bmatrix}
\in R^{2l \times p}.
\]

The decoding process includes only one layer - Answer Pointer layer. This layer is motivated by the Pointer Net in \cite{vinyals2015pointer} discussed in Section \ref{sect:pointerNet}. Wang and Jiang proposed two ways to design this layer. Here I only explain the boundary way. In this way, each output of the decoding process includes two probability distributions. The first probability distribution tells how likely each token in passage to be the start of the answer. The second probability distribution tells how likely each token in passage to be the end of the answer. To be specific,

$$F_k = tahn(VH^r + (W^a{h^a_{k-1}} +  b^a) \otimes e_p)$$
$$\beta _k = softmax(v^tF_k + c \otimes e_p)$$


where
$$V \in R^{l \times 2l}$$
$$W^a\in R^{l \times l} $$
$$b_a, v\in R^{l}  $$
$$c \in R $$
$${h_{k-1}}^a\in R^{l}: ith\ state\ of\ answer-LSTM  $$

and answer-LSTM is


$${h_k}^a = LSTM(H^r\beta _k^T, h_{k-1}^a)$$

By iterating between the attention mechanism and the answer-LSTM two times, we could get the output of the decoding process - $\beta _0$ and $\beta _1$.


Then we can get the loss function. Let $a_s$ denote the ground truth start index of the answer and $a_e$ denote the ground truth end index, we have

$$p(a|H^r) = p(a_s|H_r)p(a_r|H_r)=\beta _{0, a_s} \times \beta_{1, a_e}$$

where $$\beta_{k, j} = jth\ token\ of\ \beta _k$$

To train the model, the loss function

$$J(\theta) = -\frac{1}{N}\sum_{i=1}^{N} \log{p(a^n|H^r)} $$

is minimized.

\section{Experiment Zero} \label{sect:change_0}

The difference from experiment zero to baseline model is in the decoding process. In experiment zero,
$${h_k}^a = H^r\beta _{k}^T.$$
That is, instead of the answer-LSTM state, the previous attention vector is used to query the current attention weight.

\section{Experiment One}

The difference between experiment one and experiment zero is in experiment one the $W^r\overrightarrow{{h_{i-1}}^r}$ in Bi-directional Match-LSTM layer is removed. This modification aims at checking whether $\overrightarrow{{h_{i-1}}^r}$ carries some redundant context information. After this change,


$$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + b^p) \otimes e_q)$$


\section{Experiment Two}

The difference between experiment two and experiment zero is in experiment two the Preprocessing layer is removed. This modification aims at checking whether Preprocessing layer carries some redundant context information.

\section{Experiment Three}

The difference between experiment three and experiment zero is in experiment three both Preprocessing layer and $W^r\overrightarrow{{h_{i-1}}^r}$ in Bi-directional Match-LSTM layer are removed. This aims at checking whether context information carried by both is included in some other place.

\chapter{Implementation}

\section{Adjusting Data to Support Batch Training}\label{sect:padding}

In practice, passages have different length. So do questions. However, in one specific model, the number of times to iterate encoding or decoding process is fixed. We are intended to use all training data to train the model instead of using a single instance. As such, adjusting all passages to a same length and adjusting all questions to another same length is necessary. For sequences longer than a fixed length, we cut some part of the sentence out. For sequences shorted than a fixed length, we use some special vector to pad them.

Due to this adjustment, the models in implementation has some difference with the theoretical ones. Below I will talking about the difference taking the baseline model as example.

Assume in data processing step, each passage is adjusted to $passage\_padding\_length$ and is paired with an vector $passage\_mask$ which has size $passage\_padding\_length$ and each question is adjusted to $question\_padding\_length$ and paired with one $question\_mask$ which has size $question\_padding\_length$. The entry of mask vector is either 0 or 1. 0 indicates the current token does not exit in original sequence. 1 indicates the opposite. When constructing the model, every effort is made to prevent the model from distracted by not existing tokens.

In preprocessing layer, after getting a sequence of hidden states, the mask vector is used to reset the value of not existing positions to zero by an additional step
$$H^p = H^p \circ (passage\_mask \otimes l)$$
$$H^q = H^q \circ (question\_mask \otimes l).$$

In match-LSTM layer, the attention weights of not existing tokens are also set to zero by an additional step
$$\overrightarrow{\alpha _i} = softmax( (w^t\overrightarrow{G_i} + b \otimes e_q) ) \circ question\_mask .$$

Similar to preprocessing layer, we have
$$H_r = H_r \circ (passage\_mask \otimes 2l).$$

In Ans-Ptr layer, similar to match-LSTM layer, we have
$$\beta _k = softmax( (v^tF_k + c \otimes e_p) ) \circ passage\_mask.$$


\section{Tensorflow Graph}

Tensorflow is an open source machine learning framework. The central idea of Tensorflow is describing a complex numeric computation as a graph.  {\tt Variables} are "trainable" nodes.  {\tt Placeholders} are nodes whose values are fed in run time. Taking the baseline model as an example,  {\tt Variables} should be used to represent all the parameters of encoding and decoding layers and {\tt Placeholders} should be used to represent passages, questions, and answers. To train a graph, we do not need to compute the gradients on our own. Instead, we call some APIs of Tensorflow to get a train operation, and then feed data through placeholders to the train operation to update the parameters.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/tf_graph.png}
  \caption{Concept of Tensorflow graphs in this project}
  \label{f:tf_graph}
\end{figure}

Figure \ref{f:tf_graph} describes the concept of the Tensorflow graphs I use in this project. The cloud shape represents the {\tt Placeholders} whose values are to be fed in run time. The {\tt Variables} are included in {\tt Encoder} and {\tt Decoder}. The rectangles represent Tensors in the calculation process. The circles represent the final Tensors we use.

Here is how to build the graph. In the embedding step, the model transforms tokens to word feature vectors using a given word feature vector matrix. This matrix is part of the graph is is constant. The {\tt Encoder} encodes vectors to $H_r$ using a lot of {\tt Variables}, and the {\tt Decoder} decoders $H_r$ using a lot of {\tt Variables} to two probability distributions $\beta _0$ and $\beta _1$. Recall that $\beta _0$ tells the likeliness of each token in passage to be the start of the answer and $\beta _1$ tells the likeliness of each token in passage to be the end of the answer. The loss is calculated using $\beta _0$, $\beta _1$ and ground truth answers feed through answer {\tt Placeholders}. From the loss, a train operation is calculated. It includes the gradients on all {\tt Variables}.

Here is how the run time works. In training and validation process, the passages, questions and answers are fed into the graph. Running the train operation with data fed into through {\tt Placeholders} will update all the {\tt Variables}. In testing process, only passages and questions are fed into the graph. The predicted answers are determined from $\beta _0$ and $\beta _1$.






\section{Implementation Pipeline}

The train and validation process produces a brunch of Tensorflow graphs. The validation loss is used to choose the best graph. Then the best graph is used to do testing.

\begin{figure}[htbp]\centering
  \includegraphics[width=11cm, height=3cm]{figures/pipeline.png}
  \caption{Implementation Pipelines}
  \label{f:pipeline}
\end{figure}


\chapter{Experiments}
\section{Data}
I use Stanford Question Answering Dataset (SQuAD) to do experiemnts. I use a Python natural language processing library {\tt nltk} to tokenize raw strings in json file into passage-question-answer triplets in the format of word token sequences. In total, I got a training set which contains 78839 question-answer pairs, a dev set which contains 8760 question-answer pairs, and a test set which contains 10570 question-answer pairs.

\begin{table}[htbp]\centering
  \caption{Data Sets}
  \label{tab:dataset}
  \begin{tabular}{|r|l|} \hline
    Set Name & Number of Instances \\ \hline\hline
    Train & \ 78839 \\
    Validation & \ 8760 \\
    Test & \ 10570 \\ \hline
  \end{tabular}
\end{table}

Then I face two ways to feed data into Tensorflow graph. The first way is to query the corresponding word vector of each token from the GloVe embedding matrix and turn the word sequences into vector sequences. The vector sequences are fed into the Tensorflow graph as {\tt Placeholders}. The second way includes several steps. A vocabulary is made from train and valid tokens and each word sequence is turned into an index sequence based on the vocabulary. Two special index is used to represent unknown token and padding token. The index sequences are fed into the Tensorflow graph as {\tt Placeholders}. At the same time, a smaller embedding matrix is made from the original GloVe embedding matrix based on the vocabulary. The unknown token is assigned an average of all the vectors of known tokens. The padding token is assigned zero vector. The index of each token in this matrix is same with that in vocabulary. The smaller embedding matrix is used as a constant value to build the Tensorflow graph. Since the index sequences require much less memory than the vector sequences especially when large data set is used, I choose the second way.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=9cm]{figures/data.png}
  \caption{Data Pipelines}
  \label{f:data}
\end{figure}

Before feeding index sequences to Tensorflow graph, the index sequences should be padded to fixed lengths, as mentioned in Section \ref{sect:padding}, and the whole training data should be split into mini batches to support stochastic gradient decent.


\section{Settings}

For passage length and question length, as showed in Figure \ref{f:passage_length} and Figure \ref{f:question_length}, I plot out their distributions and find out that 400 is a reasonable cut point for passage, and 30 is a reasonable cut point for question. The dimension of GloVe word vectors is set as 100. The size of hidden units is set as 64. The regularization scale of L2-regularization is set as 0.001. Tshe batch size is set as 32. The adam optimizer is set using the default settings of Tensorflow. The normalization boundary to clip gradients is set as 5. Two hundred sample instances from train set are used to estimate train accuracy. Two hundred sample instances from validation set are used to estimate validation accuracy. The learning rate is selected through several experiments, since it is the most important parameters. According to Figure \ref{f:lr}, setting learning rate to 0.002 makes the train loss descending most fast and keep descending in the first epoch.

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/passage_length.png}
  \caption{Passage Length Distribution}
  \label{f:passage_length}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/question_length.png}
  \caption{Question Length Distribution}
  \label{f:question_length}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/lr.png}
  \caption{Performance of Different Learning Rates}
  \label{f:lr}
\end{figure}

\begin{table}[htbp]\centering
  \caption{Experimental Settings}
  \label{tab:settings}
  \begin{tabular}{|r|l|} \hline
    Hyperparameter Name& Value \\ \hline\hline
    WFV Dimension (d) & \ 100 \\
    Hidden State Size (l) & \ 64 \\
    L2\_regularization Scale & \ 10570\\
    Hidden State Size (l) & \ 64\\
    Batch Size & \ 64\\
    Passage Length & \ 400\\
    Question Length & \ 30\\
    Clip Norm & \ 5\\
    Learning Rate & \ 0.002 \\ \hline
  \end{tabular}
\end{table}

I used F1 score and exact match score to evaluate the performance of each architecture. F1 treats a predicted answer and a ground truth as bag of words and calculate a harmonic average of precision and recall ; exact match measures the percentage of exactly same predictions and ground truths. The testing data contains several ground truth answers for one passage-question pair. The best score is chose as the final score.

I used Tesla K80 12 GB Memory 61 GB RAM 100 GB SSD GPU to train the four models.

\section{Results}
\subsection{Training Process}

Figure \ref{f:mlap}, \ref{f:baseline_change0}, \ref{f:baseline_change1}, \ref{f:baseline_change2} and \ref{f:baseline_change3} show the training process of the baseline match lstm and answer pointer model, baseline change zero model, baseline change one model, baseline change two model and  baseline change three model. One epoch contains roughly 25 * 100 mini batches. The train loss on the 200 sample train instances and the valid loss on the 200 sample valid instances are calculated every 100 mini batches. So do sample train scores and sample validation scores. Training one epoch roughly costs 100 minutes. A through training of each model requires around 10 epochs and costs around 17 hours.

As indicated by Figure \ref{f:mlap}, the baseline model converges after 3 epochs. The sample train f1 score converges to 0.5, the sample train exact match score converges to 0.35, the sample valid f1 score converges to 0.3, and the sample valid exact match score converges to 0.2.

As indicated by Figure \ref{f:baseline_change0}, the baseline change 0 model keeps learning in 10 epochs. The sample train f1 score converges to 0.6, the sample train exact match score converges to 0.5, the sample valid f1 score converges to 0.4, and the sample valid exact match score converges to 0.3.

As indicated by Figure \ref{f:baseline_change1}, the baseline change 1 model converges after 10 epochs. The sample train f1 score converges to 0.65, the sample train exact match score converges to 0.45, the sample valid f1 score converges to 0.4, and the sample valid exact match score converges to 0.3.

As indicated by Figure \ref{f:baseline_change2}, the baseline change 2 model converges after 4 epochs. The sample train f1 score converges to 0.6, the sample train exact match score converges to 0.4, the sample valid f1 score converges to 0.4, and the sample valid exact match score converges to 0.2.

As indicated by Figure \ref{f:baseline_change3}, the baseline change 3 model keeps learning in the 10 epochs. The sample train f1 score converges to 0.5, the sample train exact match score converges to 0.35, the sample valid f1 score converges to 0.3, and the sample valid exact match score converges to 0.2.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_corrected.png}
  \caption{Training process of baseline MLAP model}
  \label{f:mlap}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_baseline.png}
  \caption{Training process of baseline change zero model}
  \label{f:baseline_change0}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change1.png}
  \caption{Training process of baseline change one model}
  \label{f:baseline_change1}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change2.png}
  \caption{Training process of baseline change two model}
  \label{f:baseline_change2}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change3.png}
  \caption{Training process of baseline change three model}
  \label{f:baseline_change3}
\end{figure}

\subsection{Testing Results}

\begin{table}[htbp]\centering
  \caption{Testing Results}
  \label{tab:test_results}
  \begin{tabular}{|c|c|c|}
    \hline
    Model& Test Exact Match & Test F1 \\
    \hline\hline
    Baseline: MLAP & \ 23.4 &\ 33.6 \\
    Baseline Change 0& \ 33.0 &\ 45.8 \\
    Baseline Change 1 & \ 33.0 &\ 46.2 \\
    Baseline Change 2 & \ 33.0 &\ 45.6 \\
    Baseline Change 3 & \ 24.3 &\ 33.9 \\
    \hline
  \end{tabular}
\end{table}

Table \ref{tab:test_results} shows the testing results of all models. For the baseline MLAP model, I only get F1 score 33.6 and exact match score 23.4. This does not reproduce the F1 score 71.2 and the exact match score 61.1 of the reference paper\cite{wang2016machine}. The baseline change 0, baseline change 1 and baseline change 2 models behave similarly with F1 score around 46 and exact match score around 33. The baseline change 3 model behaves worse the baseline change 0, 1 and 2 with F1 score 33.9 and exact match score 24.3.




\section{Analysis}

Comparing testing results of the baseline model with that of the original paper, the difference is quite surprising. To find out why my implementation of baseline model does not reproduce the results of original paper, further debug, parameter tuning and training are needed.

Comparing testing results change 0 model with baseline model, the scores increase. As I mentioned in Section \ref{sect:change_0}, the difference of two models is whether using state of decoding lstm or a weighted average of inputs to query the next attention weights. The subtle difference is whether adding one more nonlinear layer. It turns out not including the decoding lstm makes the model perform better.

Comparing the testing results of change 1 and 2 with that of change 0, I found remove either preprocessing layer or the $h_r$ in bidirectional match lstm layer does not decrease the model performance. A reasonable guess is the two provides duplicate context information.

Comparing the testing results of change 3 with that of change 0, I found removing both preprocessing layer and the $h_r$ in bidirectional match lstm layer decreases performance a lot. This mean the context information provides by these two parts are not provide in other place of the change 0 model. As such, one of them must be kept.





\chapter{Conclusion}

This project presented a through implementation of a Question Answering (QA) system. Several models are tried. The baseline model is implemented to reproduce the results of the paper it refers to. Four changes on the baseline models are tried to understand the baseline model. The change 0 model outperforms the baseline model. Then change 1 and change 2 models have simplified structure but similar testing results with the change 0 model. By trying change 3 model it is shown change 1 and change 2 cannot be made simultaneously.

Further work is required to find out why the baseline model fails to reproduce the testing results of the reference paper. At the same time, more parameter tuning and training are necessary to make the experiments more precise. The last but not the least, making novel architecture to bypass the state-of-art results are always a good way to move the research of question answering forward.


% if you want to keep macros or chapters in separate files
% you can do that and include them with \input like this:

%\input macros.tex
%\input ch1.tex
%\input ch2.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%  Bibliography %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{amsalpha}	% or "siam", or "alpha", or "abbrv"
				% see other styles in
				% texmf/bibtex/bst

%\nocite{*}		% uncomment to list all refs in database,
			% cited or not.

\bibliography{refs}		% assumes bib database in "refs.bib"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%  Appendices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix	% don't forget this line if you have appendices!

% \chapter{Gratuitous Appendix}
% Nothing to see here.

% %\input appA.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%   THE END   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
