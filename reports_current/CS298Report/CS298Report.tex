%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  Example usage of sjsuthesis.cls %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[modernstyle,12pt]{sjsuthesis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%    load any packages which are needed    %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% these are typical

\usepackage{latexsym}		% to get LASY symbols
\usepackage{epsfig}		% to insert PostScript figures
\usepackage{graphicx}           % to insert any other kind of figure

% these are for math stuff

\usepackage{amsmath}	% AMS math features (e.g., eqn alignment)
\usepackage{amssymb}	% Various weird symbols
\usepackage{amsfonts}	% Various useful math fonts
\usepackage{amsthm}	% Fancy theorem-type environments

% Convention: everything (except pictures) is numbered inside a single
% sequence, starting again in each section.  This makes things much
% easier to read.

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem*{main}{Main Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exmp}[thm]{Example}
\newtheorem{ques}[thm]{Question}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%       all the preamble material:       %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{A Question Answering System Using Encoder-Decoder Sequence-to-sequence Recurrent Neural Networks}

\author{BO}{LI}

\degree{Master of Science}		%  #1 {long descr.}
	{M.S., Computer Science}		%  #2 {short descr.}

\degreemonth{May}
\degreeyear{2018}

\dept{Department of}			%  #1 {designation}
     {Computer Science}		        %  #2 {name}

\advisor{Dr.}				%  #1 {title}
	{Chris Pollett}			%  #2 {name}
\advisorOrg{Department of Computer Science}

\reader{Dr.~Suneuy Kim}		        %  2nd person to sign thesis
\readerOrg{Department of Computer Science}

\readerThree{Dr.~David Taylor}		%  3rd person to sign thesis
\readerThreeOrg{Department of Computer Science}

% you can optionally add \readerFour and \readerFive as well

%\readerFour{Dr.~Who Dat}		%  4th person to sign thesis
%\readerFourOrg{Department of Physics, Harvard Univ.}

% NOTE: to get the front matter single spaced, put \singlespacing
% at the start of the abstract text

\abstract{Question Answering is about making computer programs that can answer natural language questions. It is one of the most challenging tasks in natural language processing. At the present, the state-of-art performance of Question Answering is produced by applying neural network models. In this project, I successfully built a Question Answering system using encoder-decode sequence-to-sequence recurrent neural networks. In total, I tried five different models including a baseline model called Match-lstm and Answer Pointer model and four experiments based on it. Through experiments, I got two interesting observations. First, the testing performance increases when querying attention using weighted encoding states instead of decoding state in Answer Pointer layer. Second, Preprocessing layer and the $h_r$ in Bidirectional Match-LSTM layer might provide duplicate context information.
}



% acknowledgements page is optional

\acknowledgements{

Thanks to my advisor Dr. Pollett for his patience, support and guidance during this project. Thanks to Dr. Kim and Dr. Taylor for their generous service in committee.
}

% the following options can be enabled or disabled

%\ToCisShort	% a 1-page Table of Contents

% Default: List of figures will be printed
% Uncomment the \emptyLoF line to skip the list of figures
%\LoFisShort	% a 1-page List of Figures
%\emptyLoF	% no List of Figures at all

%\LoTisShort	% a 1-page List of Tables
% \emptyLoT	% no List of Tables at all


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%       BEGIN DOCUMENT...         %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% the \begin{document} will generate all the prologue material
% (signature page, TOC, etc.); if you want to control this
% behavior, uncomment one of the following lines:
%
% \SuspendPrologue    % disables the prologue entirely
% \SimplePrologue     % prints only title, abstract, TOC, TOF


% the following command will cause a draft version and the
% current date to be printed in the header area
%
% \draftVersion{1}


\begin{document}

\raggedright          % as per SJSU GS&R guidelines June 2010
\parindent=30pt       % restores indentation

% \singlespacing      % uncomment to print single spaced (e.g., for drafts)


% document body goes here

\chapter{Introduction}

Question Answering (QA) is about making computer programs that can answer natural language questions. QA techniques are widely used among search engines, personal assistant applications on smart phones, voice control systems and a lot more other applications. QA is one of the most challenging tasks in natural language processing. In recent years, more neural network models have been built to do natural language processing tasks. This approach gives more accurate result than traditional solutions, which use syntactic and semantic analysis and hand made features. Among various neural network architectures, encoder-decoder sequence-to-sequence recurrent neural networks are quite suitable for many natural language processing tasks. Such networks encode an input sequence to some vectors and then decode them to an output sequence. Since QA is also a sequence-to-sequence task, researching on how to apply the encoder-decoder sequence-to-sequence recurrent neural networks to do QA is meaningful.


In this project, I successfully built a QA system using five different models. The baseline model is the Match-lstm and Answer pointer model which is designed by Wang and Jiang\cite{wang2016machine}. The model has a typical encoder-decode sequence-to-sequence recurrent network architecture and has a network size which is not too big to train using the limited computation resource I can access. Based on the baseline model, I designed four experiments to understand it and try to improve it. Through comparing the results of baseline model and 4 experiments, I got two interesting observations. First, the performance improves when querying attention using a weighted average of encoding states instead of a decoding lstm state in Answer Pointer layer. Second, Preprocessing layer and the $h_r$ in Bidirecional Match-LSTM layer might provide duplicate context information.


The dataset used in this project is the Stanford Question Answering Dataset (SQuAD). As described by \cite{rajpurkar2016squad}, SQuAD includes questions asked by human beings on Wikipedia articles. The answer to each question is a segment of the corresponding Wikipedia article. In total, SQuAD contains 100,000+ question-answer pairs on 500+ articles.




\chapter{Background}\label{chap:background}
\section{Word Feature Vector}

Word Feature Vector (WFV) was firstly came up with by Bengio, Yoshua and Ducharme in \cite{bengio2003neural}. A word feature vector represents a word according to its relationship with other words in the vocabulary. The distance from a word feature vector to any other word feature vector tells how likely the two words appear in a same context.

The word feature vectors of the vocabulary from a given text are learned from training a neural probabilistic language model (NPLM) on the text. Denote V as the vocabulary, $w_t$ as a word from $V$, and the matrix $C$ as the word feature vectors of all words in $V$. Each instance of the training set is a sequence of words $w_1,...,w_T$ which is a segment of the text. The purpose of NPLM is to train a model $f$ such that

$$ f(w_t, ..., w_{t-n+1}) = \hat{P}(w_t | w_{t-1},...,w_{t-n+1}).$$

The computation of $f(w_t, ..., w_{t-n+1})$ is divided into two parts.
First, each $w$ is mapped to a WFV by selecting the corresponding row in $C$ to get

$$x=(C(w_{t-1}),... ,C(w_{t-n+1})).$$

Second, we get $f(w_t, ..., w_{t-n+1})$ through

$$y=b+W\cdot x + U\cdot tanh(d + H\cdot x)$$

and

$$ f(w_t, ..., w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}^{}e^{y_i}}.$$

The loss function to minimize is $$L = -\frac{1}{T}\sum _{t}^{} \log{f(w_t, ..., w_{t-n+1})}.$$


WFV enables learning dependencies on longer sentences using reasonable computation resource and time. In comparison, although n-grams could learn long term dependency by choosing large n, in practice, current computation resource could only support $n$ to be around 3. As such, a natural language model built on WFV can consider more context information than that built on n-grams. More context generally brings more accurate results.

The usage of WFV is far beyond simply predicting a word's neighbours. In practice, WFV is a common way to represent the words in a neural network model which does a natural language processing task.

In this project, I used WFV to initialize passage and question tokens.

\section{Recurrent Neural Networks (RNNs)}\label{sect:rnn}

Recurrent neural networks (RNNs) \cite{rumelhart1986learning} are used for modeling sequential data. Figure \ref{f:rnnWithNoOutputs} shows a simple recurrent network with no outputs. $x$ is the input. $h$ is the state. $\theta$ is the hyperparameter. The relation between $h$ and $x$ is

$$h_t = f(h_{t-1}, x_t; \theta).$$

An example of $f$ is

$$h_t = sigmoid(W_h h_{t-1} + W_x x_t + b).$$

\begin{figure}[htbp]\centering
  \includegraphics[width=9cm, height=3cm]{figures/rnnWithNoOutputs}
  \caption{A simple recurrent network}
  \label{f:rnnWithNoOutputs}
\end{figure}



Despite the fitness of applying RNNs to sequential data, vanishing problem exists. Vanishing means the gradients become smaller and smaller as the network going forward. When this happens, the networks are learning very slow or even stop learning. The main solution to vanishing problem is using a more complex learning unit. In 1997, hochreiter invented Long Short Term Momory (LSTM) cell \cite{hochreiter1997long} which decreases the vanishing problem. LSTM has one more memory cell to remember long term context and use forget gate, input gate and output gate to control how much information to flow into and out of the current unit. Aside from LSTM, Cho et al. invented Gated Recurrent Unit (GRU)\cite{cho2014learning} which has a simplified structure but similar function with LSTM.


In this project, I used LSTM and GRU equally as learning unit. Among various RNN structures, I mainly use two types. The first type is a RNN with recurrent connections between hidden states as Figure \ref{f:rnnWithNoOutputs}. The sequence of  states are needed. The second type is also a RNN with recurrent connections between hidden states. However, the last state is needed.

\section{Bidirectional RNNs}

The RNNs of Section \ref{sect:rnn}
iterate from left to right. As such, the $h_t$ only contains context information from $x_1$ to $x_t$, but does not contain context information from $x_{t+1}$ to the end. However, in most sequence-to-sequence tasks, $h_t$ should contain the information of the whole sequence. Bidirectional RNNs make this possible. In a bidirectional RNN, one cell rolls from left to right, and another cell rolls from right to left. As illustrated in Figure \ref{f:bidirectionalRnn}, at time t, using both $h_t$ and $g_t$ can get context information of the whole sequence.

\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=5cm]{figures/bidirectionalRnn.png}
  \caption{A simple bidirectional recurrent neural network}
  \label{f:bidirectionalRnn}
\end{figure}

In this project, I used bidirectional RNNs in encoding part.

\section{Encoder-Decoder Sequence-to-Sequence Architecture}

Sequence-to-sequence means the input to the model is a sequence and the output from the model is also a sequence. An encoder-decoder architecture can be applied to do this task. The process of understanding the input sequence is considered as encoding the input sequence to some vectors $Crypto$. The process of generating output is considered as decoding the $Crypto$. Figure \ref{f:encoderDecoder} shows the concept of encoder-decoder sequence-to-sequence architecture . $x$ is the input, $h$ is the state in encoding process, $y$ is the output, and $g$ is the state of decoding process.

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/encoderDecoder.png}
  \caption{The concept of encoder-decoder sequence-to-sequence architecture}
  \label{f:encoderDecoder}
\end{figure}

The question answering task in this project is a sequence-to-sequence task. However, some additional techniques must be equipped to the basic architecture in Figure \ref{f:encoderDecoder}. Each input actually includes two sequences - a question and a passage. As such, in the encoding process, some method is required to make each passage aware of the corresponding question and encode them together. The attention mechanism discussed in Section \ref{sect:attention} is one such method. At the same time, each output sequence is an answer which is represented by two indices of the input passage sequence. A special decoding technique called Pointer Network discussed in Section \ref{sect:pointerNet} is needed.

\section{Attention Mechanism}\label{sect:attention}

Attention mechanism was firstly came up with by Bahdanau et al. \cite{bahdanau2014neural} in the application of neural machine translation. In the neural machine translation task, an encoder-decoder sequence-to-sequence model encodes each input sentence to some vectors and decodes the vectors to a sentence in another language with the same content. The attention mechanism was used to enable the decoding process know about the encoding states $h$. As shown in Figure \ref{f:attention}, $y$ is output, $g$ is state, and $c$ is attention vector. We have
$$g_i =f(g_{i-1},y_{i-1},c_i).$$
The attention vector $c_i$ is produced by using $g_{i-1}$ to ``query'' the encoding states $h_1, ... h_n$ through
$$c_i = \sum _j {\alpha _{i,j} h_j}$$
$$\alpha _{i,j} = \exp{e_{i,j}} / \sum _j {\exp{e_{i,j}}}$$
$$e_{i,j} = attention(h_j, g_{i-1}).$$
An example of the $attention$ function is $e_{i,j} = tanh(W_h h_j + W_g g_{i-1} + b)$.


\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=8cm]{figures/attention}
  \caption{Attention mechanism in machine translation}
  \label{f:attention}
\end{figure}



Indeed, attention mechanism is a way to ``be aware of a sequence''. Since being aware of more context is a basic need of natural language processing tasks, it is reasonable to use attention mechanism in other tasks besides machine translation.

In this project, the passage is allowed to ``be aware of the question'' in encoding process. The detailed formulas is given in Chapter \ref{chap:design}. At the same time, the answer is allowed to ``be aware of the encoding states of passage and question''. The detailed formulas is also given in Chapter \ref{chap:design}.




\section{Pointer Network}\label{sect:pointerNet}

Pointer Network\cite{vinyals2015pointer} was invented by Vinyals et al. in 2015. Pointer Network enables decoder to output tokens from input sequence. Attention mechanism is used in pointer network. However, aside from getting an attention vector, the attention weight $\alpha$ is considered as a probability distribution which indicates how likely each token in input sequence is the current output. That is,
$$y_i = x_k$$
where
$$k = argmax_j(\alpha _{i,j}).$$
Note that compared with the machine translation architecture \ref{f:attention}, in pointer network, $y_i$ is not fed into the next decoding state.


\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=8cm]{figures/pointerNetwork.png}
  \caption{The concept of pointer network}
  \label{f:pointerNetwork}
\end{figure}

In this project, the decoding part of model used Pointer Network.


\chapter{Design}\label{chap:design}

In this chapter, I will review the baseline model - Match-LSTM and Answer Pointer model and four experiments based on the baseline model. The baseline model is quite suitable for this project. First, it reflects a general encoder-decoder sequence-to-sequence architecture to do question answering tasks. Second, its network size is not too large to train given the limited computation resource I can access.


\section{Baseline Model: Match-LSTM and Answer Pointer (MLAP) }

Wang and Jiang  proposed an encoder-decoder sequence-to-sequence architecture for the question answering task on SQuAD dataset \cite{wang2016machine}. Each instance of training data includes one passage, one question and one answer. The passage is a sequence of tokens, the question is a sequence of tokens, and the answer is two indices indicating the start and end positions in passage. Recall that each answer is part of the corresponding passage in SQuAD.

Before feeding training data into model, tokens in passages and questions are vectorized to word feature vectors. As such, some pre-trained word feature vector matrix is an additional dataset in need.

The vectorized training data is fed into the encoder. The encoder includes two layers - Preprocessing layer and Bi-directional Match-LSTM layer. In Preprossing layer, a LSTM network runs over each passage word feature vector sequence and outputs a sequence of LSTM states. The same LSTM is used to encode each question word vector sequence to a sequence of LSTM states.

$$H^p = \overrightarrow{LSTM}(P)$$
$$H^q = \overrightarrow{LSTM}(Q)$$

where

 $$P\in R^{d \times p}: passage$$
 $$Q\in R^{d \times q}: question$$
 $$H^p\in R^{l \times p}: encoded\ passage$$
 $$H^q\in R^{l \times q}: encoded\ question$$
 $$p: length \ of\ passage$$
 $$q: length\ of\ question$$
 $$l: dimension\ of\ LSTM\ states$$
 $$d: dimension\ of\ word\ feature\ vector$$

In Bi-directional Match-LSTM layer, a LSTM equipped with passage-question attention, which is called Match-LSTM, is used to encode each sequence of passage states and the corresponding sequence of question states together to a sequence of Match-LSTM states. To be specific,

$$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + W^r\overrightarrow{{h_{i-1}}^r} + b^p) \otimes e_q)$$
$$\overrightarrow{\alpha _i} = softmax(w^t\overrightarrow{G_i} + b \otimes e_q)$$


where

$$W^q, W^p, W^r\in R^{l \times l} $$
$$b_p, w\in R^{l}  $$
$$b \in R $$
$${{h_{i}}pr}\in R^{l}: one\ column\ of\ H^p  $$

and

\[ \overrightarrow{z_i} =
\begin{bmatrix}
{h_i}^p \\
H^q\overrightarrow{ {\alpha _i}}^T \\
\end{bmatrix}
\in R^{2l}
\]
$$\overrightarrow{{h_i}^r} = \overrightarrow{LSTM}(\overrightarrow{z_i}, \overrightarrow{{h_{i-1}}^r}).$$

After iterating between getting attention vector $\overrightarrow{\alpha _i}$ and getting Match-LSTM state ${{h_{i}}^r}$ $p$ times, we get $[{{h_{1}}^r}, ..., {{h_{p}}^r}]$. Concatenate them to get

$$\overrightarrow{H^r} = [{{h_{1}}^r}, ..., {{h_{p}}^r}] \in R^{l \times p}.$$

Then go over $H^p$ from right to left to get $\overleftarrow{H^r}$. Concatenate $\overrightarrow{H^r}$ and $\overleftarrow{H^r}$ to get the final output of encoding process

\[ H^r =
\begin{bmatrix}
\overrightarrow{H^r} \\
\overleftarrow{H^r} \\
\end{bmatrix}
\in R^{2l \times p}.
\]

The decoding process includes only one layer - Answer Pointer layer. This layer is motivated by the Pointer Net in \cite{vinyals2015pointer} discussed in Section \ref{sect:pointerNet}. Wang and Jiang proposed two ways to design this layer. Here I only explain the boundary way. In this way, each output of the decoding process includes two probability distributions. The first probability distribution tells how likely each token in passage to be the start of the answer. The second probability distribution tells how likely each token in passage to be the end of the answer. To be specific,

$$F_k = tahn(VH^r + (W^a{h^a_{k-1}} +  b^a) \otimes e_p)$$
$$\beta _k = softmax(v^tF_k + c \otimes e_p)$$


where
$$V \in R^{l \times 2l}$$
$$W^a\in R^{l \times l} $$
$$b_a, v\in R^{l}  $$
$$c \in R $$
$${h_{k-1}}^a\in R^{l}: ith\ state\ of\ answer-LSTM  $$

and answer-LSTM is


$${h_k}^a = LSTM(H^r\beta _k^T, h_{k-1}^a)$$

By iterating between the attention mechanism and the answer-LSTM two times, we could get the output of the decoding process - $\beta _0$ and $\beta _1$.


Then we can get the loss function. Let $a_s$ denote the ground truth start index of the answer and $a_e$ denote the ground truth end index, we have

$$p(a|H^r) = p(a_s|H_r)p(a_r|H_r)=\beta _{0, a_s} \times \beta_{1, a_e}$$

where $$\beta_{k, j} = jth\ token\ of\ \beta _k$$

To train the model, the loss function

$$J(\theta) = -\frac{1}{N}\sum_{i=1}^{N} \log{p(a^n|H^r)} $$

is minimized.

\section{Experiment Zero} \label{sect:change_0}

The difference from experiment zero to baseline model is in the decoding process. In experiment zero,
$${h_k}^a = H^r\beta _{k}^T.$$
That is, instead of the answer-LSTM state, the previous attention vector is used to query the current attention weight.

\section{Experiment One}

The difference between experiment one and experiment zero is in experiment one the $W^r\overrightarrow{{h_{i-1}}^r}$ in Bi-directional Match-LSTM layer is removed. This modification aims at checking whether $\overrightarrow{{h_{i-1}}^r}$ carries some redundant context information. After this change,


$$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + b^p) \otimes e_q)$$


\section{Experiment Two}

The difference between experiment two and experiment zero is in experiment two the Preprocessing layer is removed. This modification aims at checking whether Preprocessing layer carries some redundant context information.

\section{Experiment Three}

The difference between experiment three and experiment zero is in experiment three both Preprocessing layer and $W^r\overrightarrow{{h_{i-1}}^r}$ in Bi-directional Match-LSTM layer are removed. This aims at checking whether context information carried by both is included in some other place.

\chapter{Implementation}

\section{Adjusting Data to Support Batch Training}\label{sect:padding}

When training a model, all training data are fed into the model to update parameters. In one specific model, the number of times to iterate encoding process is fixed. However, passages have different lengths and questions also have different lengths. As such, adjusting all passages to a same length and adjusting all questions to another same length are necessary.

For sequences longer than a fixed length, some part of the sentence is cut out. For sequences shorter than a fixed length, a faking pad token is used to pad them. In practice, each passage is adjusted to $passage\_padding\_length$ and is paired with a mask vector $passage\_mask$ which has size $passage\_padding\_length$. Each question is adjusted to $question\_padding\_length$ and paired with a mask vector $question\_mask$ which has size $question\_padding\_length$. Each entry of mask vector is either 0 or 1. 0 indicates the current token does not exit in the original sequence. 1 indicates the opposite.

When implementing a model, every effort is made to prevent the model from distracted by not existing tokens. This makes the model in implementation different from the theoretical one. Take the baseline model as example. In Preprocessing layer, after getting a sequence of states, the mask vector is used to reset the values of not existing positions to zero by an additional step
$$H^p = H^p \circ (passage\_mask \otimes l)$$
$$H^q = H^q \circ (question\_mask \otimes l).$$
In Match-LSTM layer, the attention weights of not existing positions are also set to zero by an additional step
$$\overrightarrow{\alpha _i} = softmax( (w^t\overrightarrow{G_i} + b \otimes e_q) ) \circ question\_mask .$$
Similar to Preprocessing layer, we have
$$H_r = H_r \circ (passage\_mask \otimes 2l).$$
In Answer Pointer layer, similar to Match-LSTM layer, we have
$$\beta _k = softmax( (v^tF_k + c \otimes e_p) ) \circ passage\_mask.$$


\section{Tensorflow Graph}

Tensorflow is an open source machine learning framework. The central idea of Tensorflow is describing a complex numeric computation as a graph.  {\tt Variables} are ``trainable'' nodes.  {\tt Placeholders} are nodes whose values are fed in run time. Taking the baseline model as an example,  {\tt Variables} should be used to represent all the parameters of encoding and decoding layers and {\tt Placeholders} should be used to represent passages, questions, and answers. To train a graph, some APIs of Tensorflow are called to get a train operation. Then the training data are fed through {\tt Placeholders} to run the train operation. When the train operation is run, the {\tt Variables} are updated.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/tf_graph.png}
  \caption{Concept of Tensorflow graphs in this project}
  \label{f:tf_graph}
\end{figure}

Figure \ref{f:tf_graph} describes the concept of the Tensorflow graphs used in this project. The cloud shapes represent the {\tt Placeholders}. The {\tt Variables} are included in {\tt Encoder} and {\tt Decoder}. The rectangles represent nodes in the calculation process. The circles represent the final outputs used in training or testing.

Here is how to build the graph. In the embedding step, the model transforms tokens to word feature vectors using a given word feature vector matrix. This matrix is part of the graph and is constant. The {\tt Encoder} encodes passage word feature vectors and question feature vectors to $H^r$. The {\tt Decoder} decoders $H^r$ to two probability distributions $\beta _0$ and $\beta _1$. Recall that $\beta _0$ tells how likely each token in a passage is the start of the answer and $\beta _1$ tells how likely each token in a passage is the end of the answer. The loss is calculated using $\beta _0$, $\beta _1$ and ground truth answers feed through answer {\tt Placeholders}. From the loss, a train operation is calculated.

Here is how the run time works. In training and validation process, the passages, questions and answers are fed into the graph. Running the train operation with data fed into through {\tt Placeholders} will update all the {\tt Variables}. In testing process, only passages and questions are fed into the graph. The predicted answers are determined from $\beta _0$ and $\beta _1$.






\section{Implementation Pipeline}

For a specific model, in the train and validation process, some Tensorflow graphs that have same structure but different values for {\tt Variables} are saved. The validation loss is used to choose the best graph. Then the best graph is used to do testing.

\begin{figure}[htbp]\centering
  \includegraphics[width=11cm, height=3cm]{figures/pipeline.png}
  \caption{Implementation Pipelines}
  \label{f:pipeline}
\end{figure}


\chapter{Experiments}
\section{Data}
The Stanford Question Answering Dataset (SQuAD) is used to do experiments. The GloVe word feature vectors\cite{pennington2014glove} are used to initialize words.

In the first step, a Python natural language processing library {\tt nltk} is used to tokenize raw data into passage word token sequences, question word token sequences and answer spans. Each answer span includes the start index and the end index. In total, the training set contains 78839 instances, the validation set contains 8760 instances, and the test set contains 10570 instances.

\begin{table}[htbp]\centering
  \caption{Data Sets}
  \label{tab:dataset}
  \begin{tabular}{|r|l|} \hline
    Set Name & Number of Instances \\ \hline\hline
    Train & \ 78839 \\
    Validation & \ 8760 \\
    Test & \ 10570 \\ \hline
  \end{tabular}
\end{table}

The vocabulary is then made from the word token sequences from training set and validation set. After the vocabulary is made, each word token is turned into its corresponding index in vocabulary. Two special index is used to represent unknown token and padding token. Before feeding index sequences to Tensorflow graph, the index sequences should be padded to fixed lengths, as mentioned in Section \ref{sect:padding}. The whole training data set should be split into mini batches to support stochastic gradient decent.

After the vocabulary is made, a smaller word feature vector matrix is made from the original GloVe word feature vectors. The smaller WFV matrix only includes vectors of word in the vocabulary. The unknown token is assigned an average of all the vectors of known tokens. The padding token is assigned zero vector. The index of each token in the smaller WFV matrix is same with that in vocabulary.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=9cm]{figures/data.png}
  \caption{Data Pipelines}
  \label{f:data}
\end{figure}




\section{Settings}

Figure \ref{f:passage_length} shows the distribution of passage lengths. Figure \ref{f:question_length} shows the distribution of question lengths. Based on the distributions, 400 is set as $passage\_padding\_length$ and 30 is set as $question\_padding\_length$. The GloVe word feature vectors with size 100 is used. The size of LSTM state in Preprocessing layer, which is $l$ in theoretical model, is set as 64. The regularization scale of L2-regularization is set as 0.001. The batch size is set as 32. The adam optimizer is set using the default settings of Tensorflow. The normalization boundary to clip gradients is set as 5. 200 sample instances from training set are used to estimate training accuracy. 200 sample instances from validation set are used to estimate validation accuracy. The learning rate is selected through several experiments shown in Figure \ref{f:lr}. Based on these experiments, the learning rate is set as 0.002.

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/passage_length.png}
  \caption{Passage Length Distribution}
  \label{f:passage_length}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/question_length.png}
  \caption{Question Length Distribution}
  \label{f:question_length}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/lr.png}
  \caption{Performance of Different Learning Rates}
  \label{f:lr}
\end{figure}

\begin{table}[htbp]\centering
  \caption{Experimental Settings}
  \label{tab:settings}
  \begin{tabular}{|r|l|} \hline
    Hyperparameter Name& Value \\ \hline\hline
    WFV Dimension (d) & \ 100 \\
    Hidden State Size (l) & \ 64 \\
    L2\_regularization Scale & \ 10570\\
    Hidden State Size (l) & \ 64\\
    Batch Size & \ 64\\
    Passage Length & \ 400\\
    Question Length & \ 30\\
    Clip Norm & \ 5\\
    Learning Rate & \ 0.002 \\ \hline
  \end{tabular}
\end{table}

F1 score and exact match score are used to evaluate the performance of each model. F1 treats a predicted answer and a ground truth as bag of words and calculate a harmonic average of precision and recall ; exact match measures the percentage of exactly same predictions and ground truths. The testing data contains several ground truth answers for one passage-question pair. The best score is chosen as the final score.

A machine that has Tesla K80 12 GB Memory, 61 GB RAM and 100 GB SSD is used to train the models.

\section{Results}
\subsection{Training Process}

Figure \ref{f:mlap}, \ref{f:baseline_change0}, \ref{f:baseline_change1}, \ref{f:baseline_change2} and \ref{f:baseline_change3} show the training process of five different models. One epoch contains roughly 25 * 100 mini batches. The training loss and training scores are calculated every 100 mini batches using the 200 sample instances from training set. So do validation loss and scores. Training one epoch roughly costs 100 minutes. A through training of each model requires around 10 epochs and takes around 17 hours.

As indicated by Figure \ref{f:mlap}, the baseline model converges after 3 epochs. The training F1 score converges to around 0.5, the training exact match score converges to around 0.35, the validation F1 score converges to around 0.3, and the validation exact match score converges to around 0.2.

As indicated by Figure \ref{f:baseline_change0}, the experiment zero model keeps learning in 10 epochs. The training F1 score converges to around 0.6, the training exact match score converges to around 0.5, the validation F1 score converges to around 0.4, and the validation exact match score converges to around 0.3.

As indicated by Figure \ref{f:baseline_change1}, the experiment one model converges after 10 epochs. The training F1 score converges to around 0.65, the training exact match score converges to around 0.45, the validation F1 score converges to around 0.4, and the validation exact match score converges to around 0.3.

As indicated by Figure \ref{f:baseline_change2}, the experiment two model converges after 4 epochs. The training F1 score converges to around 0.6, the training exact match score converges to around 0.4, the validation F1 score converges to around 0.4, and the validation exact match score converges to around 0.2.

As indicated by Figure \ref{f:baseline_change3}, the experiment three model keeps learning in the 10 epochs. The training F1 score converges to around 0.5, the training exact match score converges to around 0.35, the validation F1 score converges to around 0.3, and the validation exact match score converges to around 0.2.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_corrected.png}
  \caption{Training process of baseline MLAP model}
  \label{f:mlap}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_baseline.png}
  \caption{Training process of baseline change zero model}
  \label{f:baseline_change0}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change1.png}
  \caption{Training process of baseline change one model}
  \label{f:baseline_change1}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change2.png}
  \caption{Training process of baseline change two model}
  \label{f:baseline_change2}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change3.png}
  \caption{Training process of baseline change three model}
  \label{f:baseline_change3}
\end{figure}

\subsection{Testing Results}

\begin{table}[htbp]\centering
  \caption{Testing Results}
  \label{tab:test_results}
  \begin{tabular}{|c|c|c|}
    \hline
    Model& Test Exact Match & Test F1 \\
    \hline\hline
    Baseline: MLAP & \ 23.4 &\ 33.6 \\
    Baseline Change 0& \ 33.0 &\ 45.8 \\
    Baseline Change 1 & \ 33.0 &\ 46.2 \\
    Baseline Change 2 & \ 33.0 &\ 45.6 \\
    Baseline Change 3 & \ 24.3 &\ 33.9 \\
    \hline
  \end{tabular}
\end{table}

Table \ref{tab:test_results} shows the testing results of all models. The baseline MLAP model gets F1 score 33.6 and exact match score 23.4. This does not reproduce the F1 score 71.2 and the exact match score 61.1 in the reference paper\cite{wang2016machine}. The experiment zero, one two models behave similarly with F1 score around 46 and exact match score around 33. The experiment 3 model behaves worse than the other three experiments with F1 score 33.9 and exact match score 24.3.




\section{Analysis}

Comparing testing results of the baseline model with that of the original paper, the difference is quite surprising. To find out why my implementation of baseline model does not reproduce the results of the original paper, further debug and parameter tuning are needed.

Comparing the testing results of experiment zero and the baseline model, the scores of experiment zero are better. As mentioned in Section \ref{sect:change_0}, experiment zero uses previous attention vector to query the current attention weights. However, the baseline model uses state of answer-LSTM to query the current attention weights. Using answer-LSTM means doing a non-linear transformation on attention vector. It turns out not including the non-linear transformation gives better results.

Experiments zero, one and two behave similarly. This means removing either Preprocessing layer or the $h_r$ in Bi-directional Match-LSTM layer does not decrease test results. A reasonable guess is the two parts provide duplicate context information.

Experiment three performs worse than experiment zero, one and two. This means removing both Preprocessing layer and the $h_r$ in Bi-directional Match-LSTM layer decreases testing results. A reasonable guess is the context information provided by these two parts is not provided in other parts of the model. As such, one of the two must be kept.





\chapter{Conclusion}

This project presents a through implementation of a Question Answering (QA) system. Several models were tried. The baseline model was implemented to reproduce the results of the paper it refers to. Four experiments on the baseline models were tried to understand the baseline model. The experiment zero model outperforms the baseline model by using attention vector instead of answer-LSTM state to query attention weights. The experiment one and experiment two models have simplified structures but similar testing results with the experiment zero model. By trying experiment three it is shown that the changes in experiment one and two cannot be made simultaneously.

Further work is required to find out why the baseline model fails to reproduce the testing results of the reference paper. At the same time, more parameter tuning are needed to make the experiments more precise. The last but not the least, making novel architecture to bypass the state-of-art results is always a good way to move the research of question answering forward.


% if you want to keep macros or chapters in separate files
% you can do that and include them with \input like this:

%\input macros.tex
%\input ch1.tex
%\input ch2.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%  Bibliography %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{amsalpha}	% or "siam", or "alpha", or "abbrv"
				% see other styles in
				% texmf/bibtex/bst

%\nocite{*}		% uncomment to list all refs in database,
			% cited or not.

\bibliography{refs}		% assumes bib database in "refs.bib"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%  Appendices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix	% don't forget this line if you have appendices!

% \chapter{Gratuitous Appendix}
% Nothing to see here.

% %\input appA.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%   THE END   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
