%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  Example usage of sjsuthesis.cls %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[modernstyle,12pt]{sjsuthesis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%    load any packages which are needed    %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% these are typical

\usepackage{latexsym}		% to get LASY symbols
\usepackage{epsfig}		% to insert PostScript figures
\usepackage{graphicx}           % to insert any other kind of figure

% these are for math stuff

\usepackage{amsmath}	% AMS math features (e.g., eqn alignment)
\usepackage{amssymb}	% Various weird symbols
\usepackage{amsfonts}	% Various useful math fonts
\usepackage{amsthm}	% Fancy theorem-type environments

% Convention: everything (except pictures) is numbered inside a single
% sequence, starting again in each section.  This makes things much
% easier to read.

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem*{main}{Main Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exmp}[thm]{Example}
\newtheorem{ques}[thm]{Question}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%       all the preamble material:       %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{A Question Answering System Using Encoder-Decoder Sequence-to-sequence Recurrent Neural Networks}

\author{BO}{LI}

\degree{Master of Science}		%  #1 {long descr.}
	{M.S., Computer Science}		%  #2 {short descr.}

\degreemonth{May}
\degreeyear{2018}

\dept{Department of}			%  #1 {designation}
     {Computer Science}		        %  #2 {name}

\advisor{Dr.}				%  #1 {title}
	{Chris Pollett}			%  #2 {name}
\advisorOrg{Department of Computer Science}

\reader{Dr.~Suneuy Kim}		        %  2nd person to sign thesis
\readerOrg{Department of Computer Science}

\readerThree{Dr.~David Taylor}		%  3rd person to sign thesis
\readerThreeOrg{Department of Computer Science}

% you can optionally add \readerFour and \readerFive as well

%\readerFour{Dr.~Who Dat}		%  4th person to sign thesis
%\readerFourOrg{Department of Physics, Harvard Univ.}

% NOTE: to get the front matter single spaced, put \singlespacing
% at the start of the abstract text

\abstract{Question answering is about making computer programs that can answer natural language questions. It is one of the most challenging tasks in the field of natural language processing. The present state-of-art question answering systems use neural network models. In this project, we successfully built a question answering system using encoder-decode sequence-to-sequence recurrent neural networks. In total, five different models were tried. Model 1 is the Match-LSTM \& Answer Pointer model. Model 2, 3, 4 and 5 were designed through making changes to model 1. Through comparing the results of different models, two interesting observations were found. First, the testing performance increases when querying attention weights using previous attention vector instead of the current answer LSTM state in the answer pointer layer. Second, the preprocessing layer and using match LSTM state to query attention weights in the bidirectional match LSTM layer might provide duplicate context information.
}



% acknowledgements page is optional

% \acknowledgements{

% Thanks to my advisor Dr. Pollett for his patience, support and guidance during this project. Thanks to Dr. Kim and Dr. Taylor for their generous service in committee.
% }

% the following options can be enabled or disabled

%\ToCisShort	% a 1-page Table of Contents

% Default: List of figures will be printed
% Uncomment the \emptyLoF line to skip the list of figures
%\LoFisShort	% a 1-page List of Figures
%\emptyLoF	% no List of Figures at all

%\LoTisShort	% a 1-page List of Tables
% \emptyLoT	% no List of Tables at all


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%       BEGIN DOCUMENT...         %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% the \begin{document} will generate all the prologue material
% (signature page, TOC, etc.); if you want to control this
% behavior, uncomment one of the following lines:
%
% \SuspendPrologue    % disables the prologue entirely
% \SimplePrologue     % prints only title, abstract, TOC, TOF


% the following command will cause a draft version and the
% current date to be printed in the header area
%
% \draftVersion{1}


\begin{document}

\raggedright          % as per SJSU GS&R guidelines June 2010
\parindent=30pt       % restores indentation

% \singlespacing      % uncomment to print single spaced (e.g., for drafts)


% document body goes here

\chapter{Introduction}

Question answering is about making computer programs that can answer natural language questions. Question answering systems are widely used among search engines, personal assistant applications on smart phones, voice control systems and various applications. Various question answering systems can be categorized into two types - open domain and close domain. Open domain means the questions can be about almost everything whereas close domain means the questions are about a specific domain. In this project, we focus on open domain question answering. To simplify the research topic, we focus on a scenario when a specific passage is assigned to a question and the answer is a segment of the passage. The Stanford Question Answering Dataset (SQuAD) used in this project exactly fits such scenario. As described by \cite{rajpurkar2016squad}, SQuAD includes questions asked by human beings on Wikipedia articles. The answer to each question is a segment of the corresponding Wikipedia passage. In total, SQuAD contains more than 100,000 question-answer pairs on more than 500 articles.

In recent years, more and more state-of-art natural language processing results are produced by using neural network models. Among various neural network architectures, encoder-decoder sequence-to-sequence recurrent neural networks are used in this project. Such networks encode an input sequence to some vectors and then decode them to an output sequence. For question answering, the input sequence includes a passage and a question and the output sequence is the answer.


We successfully built a question answering system using five different models. Model 1 is Match-LSTM \& Answer Pointer model designed by Wang and Jiang\cite{wang2016machine}. The model has a typical encoder-decoder sequence-to-sequence recurrent network architecture and has a network size which is not too big to train. Model 2 to model 5 are designed by making changes to model 1. Through comparing the results of five different models, we got two interesting observations. First, the testing performance increases when querying attention weights using previous attention vector instead of the current answer LSTM state in the answer pointer layer. Second, the preprocessing layer and using match LSTM state to query attention weights in the bidirectional match LSTM layer might provide duplicate context information.






\chapter{Background}\label{chap:background}
\section{Word Feature Vector}

Word Feature Vector was firstly come up with by Bengio, Yoshua and Ducharme in \cite{bengio2003neural}. A word feature vector represents a word according to its relationship with other words in the vocabulary. The distance from one word feature vector to any other word feature vector tells how likely the two words appear in a same context.

The word feature vectors for the vocabulary of a given text are learned by training a neural probabilistic language model on the text. Denote V as the vocabulary, $w_t$ as a word from $V$, and the matrix $C$ as the word feature vectors of all words in $V$. Each instance of the training set is a sequence of words $w_1,...,w_T$ which is a segment of the text. The purpose of neural probabilistic language model is to learn a model $f$ such that

$$ f(w_t, ..., w_{t-n+1}) = \hat{P}(w_t | w_{t-1},...,w_{t-n+1}).$$

The computation of $f(w_t, ..., w_{t-n+1})$ is divided into two parts.
First, each $w$ is mapped to a word feature vector by selecting the corresponding row in $C$ to get

$$x=(C(w_{t-1}),... ,C(w_{t-n+1})).$$

Second, we get $f(w_t, ..., w_{t-n+1})$ through

$$y=b+W\cdot x + U\cdot tanh(d + H\cdot x)$$

and

$$ f(w_t, ..., w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}^{}e^{y_i}}.$$

The loss function to minimize is $$L = -\frac{1}{T}\sum _{t}^{} \log{f(w_t, ..., w_{t-n+1})}.$$


Using word feature vectors together with neural network models enables learning dependencies on long sentences. In the neural probabilistic language model, the word feature vectors are used to predict the following word after a sequence. However, the usage of word feature vectors is far beyond this. Using word feature vectors to represent words is common when applying neural network models on natural language processing tasks. This is how we use word feature vectors in this project.


\section{Recurrent Neural Networks}\label{sect:rnn}

Recurrent neural networks (RNNs) \cite{rumelhart1986learning} are used for modeling sequential data. Figure \ref{f:rnnWithNoOutputs} shows a simple recurrent network with no outputs. $x$ is the input. $h$ is the state. $\theta$ is the hyperparameter. The relation between $h$ and $x$ is

$$h_t = f(h_{t-1}, x_t; \theta).$$

An example of $f$ is

$$h_t = sigmoid(W_h h_{t-1} + W_x x_t + b).$$

\begin{figure}[htbp]\centering
  \includegraphics[width=9cm, height=3cm]{figures/rnnWithNoOutputs}
  \caption{A simple recurrent neural network}
  \label{f:rnnWithNoOutputs}
\end{figure}



Despite the fitness of applying RNNs to sequential data, the vanishing problem exists. Vanishing means the gradients become smaller and smaller as the network goes forward. When this happens, the networks are learning slowly or even stop learning. The main solution to the vanishing problem is using a more complex learning unit. In 1997, hochreiter invented Long Short Term Memory (LSTM) cell \cite{hochreiter1997long} which decreases the vanishing problem. LSTM has one more memory cell to remember long term context and use a forget gate, a input gate and a output gate to control how much information to flow into and out of the current unit. Aside from LSTM, Cho et al. invented Gated Recurrent Unit (GRU)\cite{cho2014learning} which has a simplified structure but similar function with LSTM.


In this project, I used LSTM and GRU equally as learning unit. Among various RNN structures, I mainly use two types. The first type is a RNN with recurrent connections between hidden states as Figure \ref{f:rnnWithNoOutputs}. The sequence of  states are needed. The second type is also a RNN with recurrent connections between hidden states. However, the last state is needed.

\section{Bidirectional Recurrent Neural Networks}

The RNNs of Section \ref{sect:rnn}
iterate from left to right. As such, the $h_t$ only contains context information from $x_1$ to $x_t$, but does not contain context information from $x_{t+1}$ to the end. However, in most sequence-to-sequence tasks, $h_t$ should contain the information of the whole sequence. Bidirectional RNNs make this possible. In a bidirectional RNN, one cell rolls from left to right, and another cell rolls from right to left. As illustrated in Figure \ref{f:bidirectionalRnn}, at time t, using both $h_t$ and $g_t$ can get context information of the whole sequence.

\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=5cm]{figures/bidirectionalRnn.png}
  \caption{A simple bidirectional recurrent neural network}
  \label{f:bidirectionalRnn}
\end{figure}

In this project, I used bidirectional RNNs in encoding part.

\section{Encoder-Decoder Sequence-to-Sequence Architecture}

Sequence-to-sequence means the input to a model is a sequence and the output from the model is also a sequence. An encoder-decoder architecture can be applied to do this task. The process of understanding the input sequence is considered as encoding the input sequence to some vectors $Crypto$. The process of generating output is considered as decoding the $Crypto$. Figure \ref{f:encoderDecoder} shows the concept of encoder-decoder sequence-to-sequence architecture . $x$ is the input, $h$ is the state in encoding process, $y$ is the output, and $g$ is the state of decoding process.

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/encoderDecoder.png}
  \caption{Concept of encoder-decoder sequence-to-sequence architecture}
  \label{f:encoderDecoder}
\end{figure}

The question answering task in this project is a sequence-to-sequence task. However, some additional techniques must be equipped to the basic architecture in Figure \ref{f:encoderDecoder}. Each input actually includes two sequences - a question and a passage. As such, in the encoding process, some method is required to make each passage aware of the corresponding question and encode them together. The attention mechanism discussed in Section \ref{sect:attention} is one such method. At the same time, each output sequence is an answer which is represented by two indices for the input passage sequence. A special decoding technique called pointer network discussed in Section \ref{sect:pointerNet} is needed.

\section{Attention Mechanism}\label{sect:attention}

Attention mechanism was firstly came up with by Bahdanau et al. \cite{bahdanau2014neural} in the application of neural machine translation. In a neural machine translation task, an encoder-decoder sequence-to-sequence model encodes each input sentence to some vectors and decodes the vectors to a sentence in another language with the same content. The attention mechanism is used to enable the decoding process aware of the encoding states $h_1, ... h_n$. As shown in Figure \ref{f:attention}, $y$ is the output, $g$ is the state, and $c$ is the attention vector. We have
$$g_i =f(g_{i-1},y_{i-1},c_i).$$
The attention vector $c_i$ is produced by using $g_{i-1}$ to ``query'' the encoding states $h_1, ... h_n$ through
$$c_i = \sum _j {\alpha _{i,j} h_j}$$
$$\alpha _{i,j} = \exp{e_{i,j}} / \sum _j {\exp{e_{i,j}}}$$
$$e_{i,j} = attention(h_j, g_{i-1}).$$
An example of the $attention$ function is $e_{i,j} = tanh(W_h h_j + W_g g_{i-1} + b)$.


\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=8cm]{figures/attention}
  \caption{Attention mechanism in machine translation}
  \label{f:attention}
\end{figure}



Indeed, attention mechanism is a way to ``be aware of a sequence''. Since being aware of more context is a basic need of natural language processing tasks, it is reasonable to use attention mechanism in other tasks besides machine translation.

In this project, the passage is required to ``be aware of the question'' in encoding process. At the same time, the answer is required to ``be aware of the encoding states of passage and question''. The detailed formulas is given in Chapter \ref{chap:design}.




\section{Pointer Network}\label{sect:pointerNet}

Pointer network\cite{vinyals2015pointer} was invented by Vinyals et al. in 2015. Using pointer network enables decoder to output tokens from input sequence. Attention mechanism is used in pointer network. However, aside from getting an attention vector, the attention weight vector $\alpha$ is considered as a probability distribution which indicates how likely each token in input sequence is the current output. That is,
$$y_i = x_k$$
where
$$k = argmax_j(\alpha _{i,j}).$$
Note that compared with the machine translation architecture \ref{f:attention}, in pointer network, $y_i$ is not fed into the next decoding state.


\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=8cm]{figures/pointerNetwork.png}
  \caption{Concept of pointer network}
  \label{f:pointerNetwork}
\end{figure}

In this project, pointer network is used in the decoding part of several models.


\chapter{Design}\label{chap:design}

In this chapter, I will explain the five models one by one. Recall that model 1 is the Match-LSTM \& Answer Pointer model which has a typical encoder-decoder sequence-to-sequence recurrent network architecture designed by Wang and Jiang\cite{wang2016machine}. Model 2 to model 5 are designed by making changes to model 1.


\section{Model 1 }

Wang and Jiang  proposed an encoder-decoder sequence-to-sequence architecture for the question answering task on SQuAD dataset \cite{wang2016machine}. Each instance of training data includes one passage, one question and one answer. The passage is a sequence of tokens, the question is a sequence of tokens, and the answer is a sequence of two indices indicating the start and end positions in passage. Recall that each answer is part of the corresponding passage in SQuAD.

Before feeding training data into model, tokens in passages and questions are vectorized to word feature vectors. As such, some pre-trained word feature vector matrix is an additional dataset in need.

The vectorized training data is fed into the encoder. The encoder includes two layers - the preprocessing layer and the bidirectional match LSTM layer. In the preprossing layer, a LSTM runs over each passage word feature vector sequence and outputs a sequence of LSTM states. The same LSTM is used to encode each question word feature vector sequence to a sequence of LSTM states.

$$H^p = \overrightarrow{LSTM}(P)$$
$$H^q = \overrightarrow{LSTM}(Q)$$

where

 $$P\in R^{d \times p}: passage$$
 $$Q\in R^{d \times q}: question$$
 $$H^p\in R^{l \times p}: encoded\ passage$$
 $$H^q\in R^{l \times q}: encoded\ question$$
 $$p: length \ of\ passage$$
 $$q: length\ of\ question$$
 $$l: dimension\ of\ LSTM\ states$$
 $$d: dimension\ of\ word\ feature\ vector$$

In the bidirectional match LSTM layer, a LSTM equipped with passage-question attention, which is called match LSTM, is used to encode each sequence of passage states and the corresponding sequence of question states together to a sequence of match LSTM states. To be specific,

$$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + W^r\overrightarrow{{h_{i-1}}^r} + b^p) \otimes e_q)$$
$$\overrightarrow{\alpha _i} = softmax(w^t\overrightarrow{G_i} + b \otimes e_q)$$


where

$$W^q, W^p, W^r\in R^{l \times l} $$
$$b_p, w\in R^{l}  $$
$$b \in R $$
$${{h_{i}}pr}\in R^{l}: one\ column\ of\ H^p  $$

and

\[ \overrightarrow{z_i} =
\begin{bmatrix}
{h_i}^p \\
H^q\overrightarrow{ {\alpha _i}}^T \\
\end{bmatrix}
\in R^{2l}
\]
$$\overrightarrow{{h_i}^r} = \overrightarrow{LSTM}(\overrightarrow{z_i}, \overrightarrow{{h_{i-1}}^r}).$$

After iterating between getting attention weight vector $\overrightarrow{\alpha _i}$ and getting match LSTM state ${{h_{i}}^r}$ $p$ times, we get $[{{h_{1}}^r}, ..., {{h_{p}}^r}]$. Concatenate them to get

$$\overrightarrow{H^r} = [{{h_{1}}^r}, ..., {{h_{p}}^r}] \in R^{l \times p}.$$

Then go over $H^p$ from right to left to get $\overleftarrow{H^r}$. Concatenate $\overrightarrow{H^r}$ and $\overleftarrow{H^r}$ to get the final output of encoding process

\[ H^r =
\begin{bmatrix}
\overrightarrow{H^r} \\
\overleftarrow{H^r} \\
\end{bmatrix}
\in R^{2l \times p}.
\]

The decoding process includes only one layer - the answer pointer layer. This layer is motivated by the pointer network in \cite{vinyals2015pointer} discussed in Section \ref{sect:pointerNet}. Wang and Jiang proposed two ways to design this layer. Here I only explain the boundary way. In this way, each output of the decoding process includes two probability distributions. The first probability distribution tells how likely each token in passage to be the start of the answer. The second probability distribution tells how likely each token in passage to be the end of the answer. To be specific,

$$F_k = tahn(VH^r + (W^a{h^a_{k-1}} +  b^a) \otimes e_p)$$
$$\beta _k = softmax(v^tF_k + c \otimes e_p)$$


where
$$V \in R^{l \times 2l}$$
$$W^a\in R^{l \times l} $$
$$b_a, v\in R^{l}  $$
$$c \in R $$
$${h_{k-1}}^a\in R^{l}: ith\ state\ of\ answer-LSTM  $$

and answer-LSTM is


$${h_k}^a = LSTM(H^r\beta _k^T, h_{k-1}^a)$$

By iterating between the attention mechanism and the answer LSTM two times, we could get the output of the decoding process - $\beta _0$ and $\beta _1$.


Then we can get the loss function. Let $a_s$ denote the ground truth start index of the answer and $a_e$ denote the ground truth end index, we have

$$p(a|H^r) = p(a_s|H_r)p(a_r|H_r)=\beta _{0, a_s} \times \beta_{1, a_e}$$

where $$\beta_{k, j} = jth\ token\ of\ \beta _k$$

To train the model, the loss function

$$J(\theta) = -\frac{1}{N}\sum_{i=1}^{N} \log{p(a^n|H^r)} $$

is minimized.

\section{Model 2} \label{sect:change_0}

The difference from model 2 to model 1 is in the decoding process. In model 2,
$${h_k}^a = H^r\beta _{k}^T.$$
That is, instead of the answer LSTM state, the previous attention vector is used to query the current attention weight vector.

\section{Model 3}

The difference between model 3 to model 2 is that in model 3 the $W^r\overrightarrow{{h_{i-1}}^r}$ in the bidirectional match LSTM layer is removed. This modification aims at checking whether $\overrightarrow{{h_{i-1}}^r}$ carries some redundant context information. After this change,


$$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + b^p) \otimes e_q)$$


\section{Model 4}

The difference between model 4 to model 2 is in model 4 the the preprocessing layer is removed. This modification aims at checking whether the preprocessing layer carries some redundant context information.

\section{Model 5}

The difference between model 5 and model 2 is in model 5 both the preprocessing layer and $W^r\overrightarrow{{h_{i-1}}^r}$ in the bidirectional match LSTM layer are removed. This aims at checking whether context information carried by both is included in some other parts of model 2.

\chapter{Implementation}

\section{Adjusting Models for Batch Training}\label{sect:padding}

When training a model, all training data are fed into the model to update parameters. In one specific model, the number of times to iterate encoding process is fixed. However, passages have different lengths and questions also have different lengths. As such, adjusting all passages to a same length and adjusting all questions to another same length are necessary.

For sequences longer than a fixed length, some part of the sentence is cut out. For sequences shorter than a fixed length, a faking pad token is used to pad them. In practice, each passage is adjusted to $passage\_padding\_length$ and is paired with a mask vector $passage\_mask$ which has size $passage\_padding\_length$. Each question is adjusted to $question\_padding\_length$ and paired with a mask vector $question\_mask$ which has size $question\_padding\_length$. Each entry of mask vector is either 0 or 1. 0 indicates the current token does not exit in the original sequence. 1 indicates the opposite.

When implementing a model, every effort is made to prevent the model from distracted by not existing tokens. This makes the model in implementation different from the theoretical one. Take model 1 as example. In the preprocessing layer, after getting a sequence of states, the mask vector is used to reset the values of not existing positions to zero by an additional step
$$H^p = H^p \circ (passage\_mask \otimes l)$$
$$H^q = H^q \circ (question\_mask \otimes l).$$
In the match LSTM layer, the attention weights of not existing positions are also set to zero by an additional step
$$\overrightarrow{\alpha _i} = softmax( (w^t\overrightarrow{G_i} + b \otimes e_q) ) \circ question\_mask .$$
Similar to the preprocessing layer, we have
$$H_r = H_r \circ (passage\_mask \otimes 2l).$$
In the answer pointer layer, similar to the match LSTM layer, we have
$$\beta _k = softmax( (v^tF_k + c \otimes e_p) ) \circ passage\_mask.$$


\section{Tensorflow Graphs}

Tensorflow is an open source machine learning framework. The central idea of Tensorflow is describing a complex numeric computation as a graph.  {\tt Variables} are ``trainable'' nodes.  {\tt Placeholders} are nodes whose values are fed in run time. Taking model 1 as an example,  {\tt Variables} should be used to represent all the parameters of encoding and decoding layers and {\tt Placeholders} should be used to represent passages, questions, and answers. To train a graph, some APIs of Tensorflow are called to get a train operation. Then the training data are fed through {\tt Placeholders} to run the train operation. When the train operation is run, the {\tt Variables} are updated.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/tf_graph.png}
  \caption{Concept of the Tensorflow graphs used in this project}
  \label{f:tf_graph}
\end{figure}

Figure \ref{f:tf_graph} describes the concept of the Tensorflow graphs for five different models used in this project. The cloud shapes represent the {\tt Placeholders}. The {\tt Variables} are included in {\tt Encoder} and {\tt Decoder}. The rectangles represent nodes in the calculation process. The circles represent the final outputs used in training or testing.

Here is how to build the graph. In the embedding step, the model transforms word tokens to word feature vectors using a given word feature vector matrix. This matrix is part of the graph and is constant. The {\tt Encoder} encodes passage word feature vectors and question feature vectors to $H^r$. The {\tt Decoder} decoders $H^r$ to two probability distributions $\beta _0$ and $\beta _1$. Recall that $\beta _0$ tells how likely each token in a passage is the start of the answer and $\beta _1$ tells how likely each token in a passage is the end of the answer. The loss is calculated using $\beta _0$, $\beta _1$ and ground truth answers feed through answer {\tt Placeholders}. From the loss, a train operation is calculated.

Here is how the run time works. In training and validation process, the passages, questions and answers are fed into the graph. Running the train operation with data fed into through {\tt Placeholders} will update all the {\tt Variables}. In testing process, only passages and questions are fed into the graph. The predicted answers are determined from $\beta _0$ and $\beta _1$.






\section{Implementation Pipeline}

For a specific model, in the train and validation process, some Tensorflow graphs that have same structure but different values for {\tt Variables} are saved. The validation loss is used to choose the best graph. Then the best graph is used to do testing.

\begin{figure}[htbp]\centering
  \includegraphics[width=11cm, height=3cm]{figures/pipeline.png}
  \caption{Implementation pipelines}
  \label{f:pipeline}
\end{figure}


\chapter{Experiments}
\section{Data}
The Stanford Question Answering Dataset (SQuAD) is used to do experiments. The GloVe word feature vectors\cite{pennington2014glove} are used to initialize words.

In the first step, a Python natural language processing library {\tt nltk} is used to tokenize raw data into passage word token sequences, question word token sequences and answer spans. Each answer span includes the start index and the end index. In total, the training set contains 78839 instances, the validation set contains 8760 instances, and the test set contains 10570 instances.

\begin{table}[htbp]\centering
  \caption{Data sets}
  \label{tab:dataset}
  \begin{tabular}{|r|l|} \hline
    Set Name & Number of Instances \\ \hline\hline
    Train & \ 78839 \\
    Validation & \ 8760 \\
    Test & \ 10570 \\ \hline
  \end{tabular}
\end{table}

The vocabulary is then made from the word token sequences from training set and validation set. After the vocabulary is made, each word token is turned into its corresponding index in vocabulary. Two special index is used to represent unknown token and padding token. Before feeding index sequences to Tensorflow graph, the index sequences should be padded to fixed lengths, as mentioned in Section \ref{sect:padding}. The whole training data set should be split into mini batches to support stochastic gradient decent.

After the vocabulary is made, a smaller word feature vector matrix is made from the original GloVe word feature vectors. The smaller WFV matrix only includes vectors of word in the vocabulary. The unknown token is assigned an average of all the vectors of known tokens. The padding token is assigned zero vector. The index of each token in the smaller WFV matrix is same with that in vocabulary.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=9cm]{figures/data.png}
  \caption{Data pipelines}
  \label{f:data}
\end{figure}




\section{Settings}

Figure \ref{f:passage_length} shows the distribution of passage lengths. Figure \ref{f:question_length} shows the distribution of question lengths. Based on the distributions, 400 is set as $passage\_padding\_length$ and 30 is set as $question\_padding\_length$. The GloVe word feature vectors with size 100 is used. The size of LSTM state in Preprocessing layer, which is $l$ in theoretical model, is set as 64. The regularization scale of L2-regularization is set as 0.001. The batch size is set as 32. The adam optimizer is set using the default settings of Tensorflow. The normalization boundary to clip gradients is set as 5. 200 sample instances from training set are used to estimate training accuracy. 200 sample instances from validation set are used to estimate validation accuracy. The learning rate is selected through several experiments shown in Figure \ref{f:lr}. Based on these experiments, the learning rate is set as 0.002.

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/passage_length.png}
  \caption{Distribution of passage lengths}
  \label{f:passage_length}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/question_length.png}
  \caption{Distribution of question lengths}
  \label{f:question_length}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/lr.png}
  \caption{Performance of different learning rates}
  \label{f:lr}
\end{figure}

\begin{table}[htbp]\centering
  \caption{Experimental settings}
  \label{tab:settings}
  \begin{tabular}{|r|l|} \hline
    Hyperparameter Name& Value \\ \hline\hline
    WFV Dimension (d) & \ 100 \\
    Hidden State Size (l) & \ 64 \\
    L2\_regularization Scale & \ 10570\\
    Hidden State Size (l) & \ 64\\
    Batch Size & \ 64\\
    Passage Length & \ 400\\
    Question Length & \ 30\\
    Clip Norm & \ 5\\
    Learning Rate & \ 0.002 \\ \hline
  \end{tabular}
\end{table}

F1 score and exact match score are used to evaluate the performance of each model. F1 treats a predicted answer and a ground truth as bag of words and calculate a harmonic average of precision and recall ; exact match measures the percentage of exactly same predictions and ground truths. The testing data contains several ground truth answers for one passage-question pair. The best score is chosen as the final score.

A machine that has Tesla K80 12 GB Memory, 61 GB RAM and 100 GB SSD is used to train the models.

\section{Results}
\subsection{Training Process}

Figure \ref{f:mlap}, \ref{f:baseline_change0}, \ref{f:baseline_change1}, \ref{f:baseline_change2} and \ref{f:baseline_change3} show the training process of five different models. One epoch contains roughly 25 * 100 mini batches. The training loss and training scores are calculated every 100 mini batches using the 200 sample instances from training set. So do validation loss and scores. Training one epoch roughly costs 100 minutes. A through training of each model requires around 10 epochs and takes around 17 hours.

As indicated by Figure \ref{f:mlap}, the baseline model converges after 3 epochs. The training F1 score converges to around 0.5, the training exact match score converges to around 0.35, the validation F1 score converges to around 0.3, and the validation exact match score converges to around 0.2.

As indicated by Figure \ref{f:baseline_change0}, the experiment zero model keeps learning in 10 epochs. The training F1 score converges to around 0.6, the training exact match score converges to around 0.5, the validation F1 score converges to around 0.4, and the validation exact match score converges to around 0.3.

As indicated by Figure \ref{f:baseline_change1}, the experiment one model converges after 10 epochs. The training F1 score converges to around 0.65, the training exact match score converges to around 0.45, the validation F1 score converges to around 0.4, and the validation exact match score converges to around 0.3.

As indicated by Figure \ref{f:baseline_change2}, the experiment two model converges after 4 epochs. The training F1 score converges to around 0.6, the training exact match score converges to around 0.4, the validation F1 score converges to around 0.4, and the validation exact match score converges to around 0.2.

As indicated by Figure \ref{f:baseline_change3}, the experiment three model keeps learning in the 10 epochs. The training F1 score converges to around 0.5, the training exact match score converges to around 0.35, the validation F1 score converges to around 0.3, and the validation exact match score converges to around 0.2.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_corrected.png}
  \caption{Training process of model 1}
  \label{f:mlap}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_baseline.png}
  \caption{Training process of model 2}
  \label{f:baseline_change0}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change1.png}
  \caption{Training process of model 3}
  \label{f:baseline_change1}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change2.png}
  \caption{Training process of model 4}
  \label{f:baseline_change2}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change3.png}
  \caption{Training process of model 5}
  \label{f:baseline_change3}
\end{figure}

\subsection{Testing Results}

\begin{table}[htbp]\centering
  \caption{Testing results}
  \label{tab:test_results}
  \begin{tabular}{|c|c|c|}
    \hline
    Model& Test Exact Match & Test F1 \\
    \hline\hline
    Baseline: MLAP & \ 23.4 &\ 33.6 \\
    Baseline Change 0& \ 33.0 &\ 45.8 \\
    Baseline Change 1 & \ 33.0 &\ 46.2 \\
    Baseline Change 2 & \ 33.0 &\ 45.6 \\
    Baseline Change 3 & \ 24.3 &\ 33.9 \\
    \hline
  \end{tabular}
\end{table}

Table \ref{tab:test_results} shows the testing results of all models. The baseline MLAP model gets F1 score 33.6 and exact match score 23.4. This does not reproduce the F1 score 71.2 and the exact match score 61.1 in the reference paper\cite{wang2016machine}. The experiment zero, one two models behave similarly with F1 score around 46 and exact match score around 33. The experiment 3 model behaves worse than the other three experiments with F1 score 33.9 and exact match score 24.3.




\section{Analysis}

Comparing testing results of the baseline model with that of the original paper, the difference is quite surprising. To find out why my implementation of baseline model does not reproduce the results of the original paper, further debug and parameter tuning are needed.

Comparing the testing results of experiment zero and the baseline model, the scores of experiment zero are better. As mentioned in Section \ref{sect:change_0}, experiment zero uses previous attention vector to query the current attention weights. However, the baseline model uses state of answer-LSTM to query the current attention weights. Using answer-LSTM means doing a non-linear transformation on attention vector. It turns out not including the non-linear transformation gives better results.

Experiments zero, one and two behave similarly. This means removing either Preprocessing layer or the $h_r$ in Bi-directional Match-LSTM layer does not decrease test results. A reasonable guess is the two parts provide duplicate context information.

Experiment three performs worse than experiment zero, one and two. This means removing both Preprocessing layer and the $h_r$ in Bi-directional Match-LSTM layer decreases testing results. A reasonable guess is the context information provided by these two parts is not provided in other parts of the model. As such, one of the two must be kept.





\chapter{Conclusion}

This project presents a through implementation of a Question Answering (QA) system. Several models were tried. The baseline model was implemented to reproduce the results of the paper it refers to. Four experiments on the baseline models were tried to understand the baseline model. The experiment zero model outperforms the baseline model by using attention vector instead of answer-LSTM state to query attention weights. The experiment one and experiment two models have simplified structures but similar testing results with the experiment zero model. By trying experiment three it is shown that the changes in experiment one and two cannot be made simultaneously.

Further work is required to find out why the baseline model fails to reproduce the testing results of the reference paper. At the same time, more parameter tuning are needed to make the experiments more precise. The last but not the least, making novel architecture to bypass the state-of-art results is always a good way to move the research of question answering forward.


% if you want to keep macros or chapters in separate files
% you can do that and include them with \input like this:

%\input macros.tex
%\input ch1.tex
%\input ch2.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%  Bibliography %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{amsalpha}	% or "siam", or "alpha", or "abbrv"
				% see other styles in
				% texmf/bibtex/bst

%\nocite{*}		% uncomment to list all refs in database,
			% cited or not.

\bibliography{refs}		% assumes bib database in "refs.bib"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%  Appendices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix	% don't forget this line if you have appendices!

% \chapter{Gratuitous Appendix}
% Nothing to see here.

% %\input appA.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%   THE END   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
