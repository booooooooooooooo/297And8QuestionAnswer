%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%  Example usage of sjsuthesis.cls %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[modernstyle,12pt]{sjsuthesis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%    load any packages which are needed    %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% these are typical

\usepackage{latexsym}		% to get LASY symbols
\usepackage{epsfig}		% to insert PostScript figures
\usepackage{graphicx}           % to insert any other kind of figure

% these are for math stuff

\usepackage{amsmath}	% AMS math features (e.g., eqn alignment)
\usepackage{amssymb}	% Various weird symbols
\usepackage{amsfonts}	% Various useful math fonts
\usepackage{amsthm}	% Fancy theorem-type environments

% Convention: everything (except pictures) is numbered inside a single
% sequence, starting again in each section.  This makes things much
% easier to read.

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem*{main}{Main Theorem}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}[thm]{Remark}
\newtheorem{exmp}[thm]{Example}
\newtheorem{ques}[thm]{Question}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%       all the preamble material:       %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{A Question Answering System Using Encoder-Decoder Sequence-to-sequence Recurrent Neural Networks}

\author{BO}{LI}

\degree{Master of Science}		%  #1 {long descr.}
	{M.S., Computer Science}		%  #2 {short descr.}

\degreemonth{May}
\degreeyear{2018}

\dept{Department of}			%  #1 {designation}
     {Computer Science}		        %  #2 {name}

\advisor{Dr.}				%  #1 {title}
	{Chris Pollett}			%  #2 {name}
\advisorOrg{Department of Computer Science}

\reader{Dr.~Suneuy Kim}		        %  2nd person to sign thesis
\readerOrg{Department of Computer Science}

\readerThree{Dr.~David Taylor}		%  3rd person to sign thesis
\readerThreeOrg{Department of Computer Science}

% you can optionally add \readerFour and \readerFive as well

%\readerFour{Dr.~Who Dat}		%  4th person to sign thesis
%\readerFourOrg{Department of Physics, Harvard Univ.}

% NOTE: to get the front matter single spaced, put \singlespacing
% at the start of the abstract text

\abstract{Question Answering (QA) is about making a computer program that could answer questions in natural language. It is one of the most challenging tasks in natural language processing. At the present, the state-of-art performance of QA is produced by applying neural network models. In this project, I successfully build a QA system using an encoder-decode recurrent neural network architecture called match-lstm and answer pointer model. Based on it, I implement four changes to understand the model and try to reduce the size of the model. I get two interesting observations. First, the performance increases when quering attention using weighted input instead of lstm state. Second, preprocessing layer and the $h_r$ in bidirecional match-lstm layer might provide duplicate context information.
}



% acknowledgements page is optional

\acknowledgements{

Thanks to my advisor Dr. Pollett for his patience, support and guidance during this project. Thanks to Dr. Kim and Dr. Taylor for their generous service in committee.
}

% the following options can be enabled or disabled

%\ToCisShort	% a 1-page Table of Contents

% Default: List of figures will be printed
% Uncomment the \emptyLoF line to skip the list of figures
%\LoFisShort	% a 1-page List of Figures
%\emptyLoF	% no List of Figures at all

%\LoTisShort	% a 1-page List of Tables
% \emptyLoT	% no List of Tables at all


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%       BEGIN DOCUMENT...         %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% the \begin{document} will generate all the prologue material
% (signature page, TOC, etc.); if you want to control this
% behavior, uncomment one of the following lines:
%
% \SuspendPrologue    % disables the prologue entirely
% \SimplePrologue     % prints only title, abstract, TOC, TOF


% the following command will cause a draft version and the
% current date to be printed in the header area
%
% \draftVersion{1}


\begin{document}

\raggedright          % as per SJSU GS&R guidelines June 2010
\parindent=30pt       % restores indentation

% \singlespacing      % uncomment to print single spaced (e.g., for drafts)


% document body goes here

\chapter{Introduction}

Question Answering(QA) is about making a computer program that could answer questions in natural language. QA techniques are widely used among search engines, personal assistant applications on smart phones, voice control systems and a lot more other applications. QA is one of the most challenging tasks in natural language processing. In recent years, more neural network models are built to do question answering tasks. This approach gives more accurate result than traditional solutions, which use syntactic and semantic analyses and hand made features. Researchers keep designing new neural network models to seek for better results, although better results are usually achieved by making more complex neural networks.

Encoder-decoder sequence-to-sequence recurrent neural networks are very efficient at modeling natural language processing tasks. Such networks encode the an input sequence to some vectors and then decode them to an output sequence. Recurrent neural networks are applicable here since they are good at processing sequence. Encoder-decoder sequence-to-sequence recurrent neural networks have been very successful at machine translation. Since question answering is also a sequence-to-sequence task, researching on how to apply the encoder-decoder sequence-to-sequence recurrent neural networks to do question answering is meaningful.


In this project, I choose the match lstm and answer pointer model which is designed by Wang and Jiang\cite{wang2016machine} as my baseline model. It is an encoder-decode sequence-to-sequence recurrent network model. Besides, the network size is not too big to train using the limited computation resource I could access. I first implement it to build a question answering system. Then I make four changes to understand it and try to improve it. The data used is the Stanford Question Answering Dataset (SQuAD). As described by \cite{rajpurkar2016squad}, it includes questions asked by human beings on Wikipedia articles. The answer to each question is a segment of the corresponding Wikipedia article. In total, SQuAD contains 100,000+ question-answer pairs on 500+ articles.

I successfully process all raw data using Python and implement the five different models using a machine learning framework Tensorflow. Through comparing the results of 5 different models, I get two interesting observations. First, the performance increases when querying attention using a weighted input instead of a lstm state. Second, preprocessing layer and the $h_r$ in bidirecional match-lstm layer might provide duplicate context information.



\chapter{Background}
\section{Word Feature Vector}

Word Feature Vector (WFC) was firstly came up by Bengio, Yoshua and Ducharme in \cite{bengio2003neural}. A word feature represents a word according to its relationship with all words in the vocabulary. This distance from a word feature vector to any other word feature vector tells how likely this word appears together with the other word.

The word feature vectors of the vocabulary from a given text is learned from training a neural probabilistic language model (NPLM) on the text. Denote V as the vocabulary, $w_t$ as a word from $V$, and the matrix $C$ as the word feature vectors of all words in $V$. Each instance of the training set is a sequence of words $w_1,...,w_T$. The purpose of NPLM is to train a model $f$ such that

$$ f(w_t, ..., w_{t-n+1}) = \hat{P}(w_t | w_{t-1},...,w_{t-n+1}).$$

The computation of $f(w_t, ..., w_{t-n+1})$ is divided into two parts.
First, we map each $w$ to a WFC by selecting the corresponding row in $C$ to get

$$x=(C(w_{t-1}),... ,C(w_{t-n+1})).$$

Second, we get $f(w_t, ..., w_{t-n+1})$ through

$$y=b+W\cdot x + U\cdot tanh(d + H\cdot x)$$

and

$$ f(w_t, ..., w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}^{}e^{y_i}}.$$

The loss function to minimize is $$L = -\frac{1}{T}\sum _{t}^{} \log{f(w_t, ..., w_{t-n+1})}.$$


WFC enables learning dependencies on longer sentences with reasonable computation resource and time. In comparison, although n-grams could lean long term dependency by choosing large n, in practice, current computation resource could only supports $n$ to be around 3. As such, a natural language model built on WFC can consider more context information than that built on n-grams. More context generally brings more accurate result.

The usage of WFC is far beyond simply predicting a word's neighbours. In practice, WFC is a common way to represent the words in a neural network model which does a natural language processing task. In this project, I use WFC to initialize each word input to models.

\section{Recurrent Neural Networks (RNNs)}\label{sect:rnn}

Recurrent neural networks (RNNs) \cite{rumelhart1986learning} are used for modeling sequential data. Figure \ref{f:rnnWithNoOutputs} is a simple recurrent network with no outputs. $x$ is the input. $h$ is the hidden state. $\theta$ is hyperparameter. The relation between $h$ and $x$ is

$$h_t = f(h_{t-1}, x_t; \theta).$$

An example of $f$ is

$$h_t = sigmoid(W_h h_{t-1} + W_x x_t + b).$$

\begin{figure}[htbp]\centering
  \includegraphics[width=9cm, height=3cm]{figures/rnnWithNoOutputs}
  \caption{A simple recurrent network}
  \label{f:rnnWithNoOutputs}
\end{figure}



Despite the fitness of RNNs to sequential data, in practice, vanishing problem troubles the usage of RNNs. Vanishing means the gradients become smaller and smaller as the network going forward. When this happens, the networks are learning very slow or even stop learning. The main solution to vanishing problem is using a more complex learning unit. In 1997, hochreiter invented Long Short Term Momory (LSTM) cell \cite{hochreiter1997long}. LSTM has one more memory cell to remember long term context and use forget gate, input gate and output gate to control how much information into and out of the current unit. Aside from LSTM, Cho et al. invented Gated Recurrent Unit (GRU)\cite{cho2014learning} which has a simplified structure but similar function with LSTM.


In this project, I use LSTM and GRU equally as learning unit. Among various RNN structures, I mainly use two types. The first type is a RNN with recurrent connections between hidden states. The sequence of hidden states are the output of the RNN. The second type is also a RNN with recurrent connections between hidden states. However, the last hidden state is the output of the RNN.

\section{Bidirectional RNNs}

The RNNs of Section \ref{sect:rnn}
iterate from left to right. As such, the $h_t$ only contains context information from $x_1$ to $x_t$, but does not contain context information from $x_{t+1}$ to the end. However, in most sequence-to-sequence tasks, we want $h_t$ to contain the information of the whole sequence. Bidirectional RNNs make this possible. In a bidirectional RNN, one RNN rolls from left to right, and another RNN rolls from right to left. As illustrated in Figure \ref{f:bidirectionalRnn}, at time t, using both $h_t$ and $g_t$ would get context information from the whole sequence.

\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=5cm]{figures/bidirectionalRnn.png}
  \caption{A simple bidirectional recurrent neural network}
  \label{f:bidirectionalRnn}
\end{figure}

In this project, I use bidirectional RNNs in encoding part.

\section{Encoder-Decoder Sequence-to-Sequence Architecture}

Sequence-to-sequence means the input to the model is a sequence, and the output from the model is also a sequence. An encoder-decoder architecture can be applied to do this task. The process of understanding the input sequence is considered as encoding the input sequence to some vectors $Crypto$. The process of generating output is considered as decoding the $Crypto$. Figure \ref{f:encoderDecoder} shows the concept of encoder-decoder architecture . The $x$s are inputs, the $h$s are states in encoding process, the $y$s are outputs, and the $g$s are states of decoding process.

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/encoderDecoder.png}
  \caption{The concept of encoder-decoder sequence to sequence architecture}
  \label{f:encoderDecoder}
\end{figure}

In this project, the question answering task is a sequence-to-sequence task. However, the input sequences are actually two sequences - questions and passages, and the output sequences are answers. As such, in the encoding process, we need some more method to make the passages aware of the questions and encode them together - the attention mechanism.

\section{Attention Mechanism}\label{sect:attention}

Attention mechanism was first came up with by Bahdanau et al. in \cite{bahdanau2014neural} in the application of neural machine translation. In the neural machine translation task, the input sequence is some words in one language, and the output sequence is the same content in another language. In the encoder-decoder sequence-to-sequence architecture for neural machine translation, a RNN encodes the input sequence to one state, and the decoder decodes the state to the output sequence. However, a big problem here is one state cannot contain all the information of a long input sequence. The attention mechanism was invented to enable the decoding process knows about the input sequence.

Figure \ref{f:attention} shows how attention mechanism works in machine translation. $y$ is output, $g$ is state, and $c$ is attention vector. When geting $g_2$, aside from $g_1$, $y_1$ and $c_2$ are also inputs. We have

$$g_2 =f(g_{1},y_{1},c_2).$$

$c_2$ is produced by using $g_1$ to ``query'' the encoding states $h_1, ... h_n$.

$$c_2 = \sum{\alpha _i h_i}$$
$$\alpha _i = \exp{e_i} / \sum{\exp{e_i}}$$
$$e_i = attention(h_i, g_1)$$

Then function $attention$ could be a feed forward step of neural network such as $e_i = tanh(W_h h_i + W_g g_1 + b)$.


\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=8cm]{figures/attention}
  \caption{Attention mechanism in machine translation}
  \label{f:attention}
\end{figure}




Although the attention mechanism was invented for doing machine translation task, it can be applied to many other tasks of natural language processing. We can simply understand ``attention'' as ``being aware of a sequence''. In machine translation, the decoding process needs to ``be aware of the encoding states''. In this project, we need the passage to ``be aware of the question'', and the answer to ``be aware of the encoding states of passage and question''. I will explain how the attention mechanism is used in this project in detail in Chapter \ref{chap:design}




\section{Pointer Network}\label{sect:pointerNet}

Pointer Network\cite{vinyals2015pointer} was invented by Vinyals et al. in 2015. Pointer Network enables decoder to output tokens from input sequence. Attention mechanism is used in pointer network. However, aside from getting a weighted average of encoding states, the weights $\alpha$s is considered as a probability distribution based on which a token from input sequence is selected as the output. Then we have $y_2 = x_k$ when $k = argmax_i(\alpha _2,i)$. Note that $y_i$ is not considered as input to produce decoding state.



\begin{figure}[htbp]\centering
  \includegraphics[width=8cm, height=8cm]{figures/pointerNetwork.png}
  \caption{The concept of pointer network}
  \label{f:pointerNetwork}
\end{figure}


\chapter{Design}\label{chap:design}

In this chapter, I will review the baseline model - match lstm and answer pointer model, and my four changes on the baseline model. The baseline model is quite suitable for a master project. First, it reflects a general encoder-decoder sequence-to-sequence RNN architecture to do question answering task. Second, its network size is not too large to train given the limited computation resource I can access.


\section{Baseline Model: Match-lstm and Answer Pointer (MLAP) }

Wang and Jiang  proposed an encoder-decoder sequence-to-sequence architecture for the question answering task on SQuAD dataset in \cite{wang2016machine}. Each instance of training data includes one passage, one question and one answer. The passage is a sequence of tokens, the question is a sequence of tokens, and the answer is two indices indicating the start and end position in passage. Recall that each answer is part of the corresponding passage in SQuAD dataset.

Before feeding training data into model, tokens in passages and questions are vectorized to word feature vectors. As such, some pre-trained word feature vector matrix is an additional dataset in need.

The vectorized training data is feed into the encoder. The encoder includes two layers - preprocessing layer and bi-directional match-LSTM layer. In preprossing layer, a LSTM network runs over each passage word feature vector sequence and outputs a sequence of LSTM hidden states. The same LSTM is used to encode each question word vector sequence to a sequence of LSTM hidden states.

$$H^p = \overrightarrow{LSTM}(P)$$
$$H^q = \overrightarrow{LSTM}(Q)$$

where

 $$P\in R^{d \times p}: passage$$
 $$Q\in R^{d \times q}: question$$
 $$H^p\in R^{l \times p}: encoded\ passage$$
 $$H^q\in R^{l \times q}: encoded\ question$$
 $$p: length \ of\ passage$$
 $$q: length\ of\ question$$
 $$l: dimension\ of\ LSTM\ hidden\ states$$
 $$d: dimension\ of\ word\ feature\ vector$$

In bi-directional match-LSTM layer, a LSTM equipped with passage-question attention, which is called match-LSTM, is used to encode each sequence passage hidden states and the pairing sequence of question hidden states together to a sequence of hidden states of the match-LSTM. To be specific,

$$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + W^r\overrightarrow{{h_{i-1}}^r} + b^p) \otimes e_q)$$
$$\overrightarrow{\alpha _i} = softmax(w^t\overrightarrow{G_i} + b \otimes e_q)$$


where

$$W^q, W^p, W^r\in R^{l \times l} $$
$$b_p, w\in R^{l}  $$
$$b \in R $$
$$\overrightarrow{{h_{i-1}}^r}\in R^{l}: one\ column\ of\ H_p  $$

and

\[ \overrightarrow{z_i} =
\begin{bmatrix}
{h_i}^p \\
H^q\overrightarrow{ {\alpha _i}}^T \\
\end{bmatrix}
\in R^{2l}
\]
$$\overrightarrow{{h_i}^r} = \overrightarrow{LSTM}(\overrightarrow{z_i}, \overrightarrow{{h_{i-1}}^r}).$$

After iterating between getting attention vector $\overrightarrow{\alpha _i}$ and getting hidden state ${{h_{i}}^r}$ $p$ times, we get $[{{h_{1}}^r}, ..., {{h_{p}}^r}]$. Concatenate them to get

$$\overrightarrow{H_r} = [{{h_{1}}^r}, ..., {{h_{p}}^r}] \in R^{l \times p}.$$

Then go over $H_p$ from right to left to get $\overleftarrow{H_r}$. Concatenate $\overrightarrow{H_r}$ and $\overleftarrow{H_r}$ to get the final output of encoding process

\[ H_r =
\begin{bmatrix}
\overrightarrow{H_r} \\
\overleftarrow{H_r} \\
\end{bmatrix}
\in R^{2l \times p}.
\]

The decoding process includes only one layer - Answer Pointer layer. This layer is motivated by the Pointer Net in \cite{vinyals2015pointer} I have discussed in Section \ref{sect:pointerNet}. Wang and Jiang proposed two ways to design this layer. Here I only explain the boundary way. In this way, each instance of the output of the decoding process is two probability distributions. The first probability tells how likely each token in passage to be the start of the answer. The second probability distribution tells how likely each token in passage to be the end of the answer. To be specific,

$$F_k = tahn(VH_r + (W^a{h^a_{k-1}} +  b^a) \otimes e_p)$$
$$\beta _k = softmax(v^tF_k + c \otimes e_p)$$


where
$$V \in R^{l \times 2l}$$
$$W^a\in R^{l \times l} $$
$$b_a, v\in R^{l}  $$
$$c \in R $$
$${h_{k-1}}^a\in R^{l}: hidden\ state\ at\ positiom\ i\ of\ answer\ LSTM  $$

and answer LSTM is


$${h_k}^a = LSTM(H^r\beta _k^T, h_{k-1}^a)$$

By iterating between the attention mechanism and the answer LSTM two times, we could get the output of the decoding process - $\beta _0$ and $\beta _1$.


Then we can get the loss function. Let $a_s$ denote the ground truth start index of the answer, and $a_e$ denote the ground truth end index, then we have

$$p(a|H^r) = p(a_s|H_r)p(a_r|H_r)=\beta _{0, a_s} \times \beta_{1, a_e}$$

where $$\beta_{k, j} = jth\ token\ of\ \beta _k$$

To train the model, the loss function

$$J(\theta) = -\frac{1}{N}\sum_{i=1}^{N} \log{p(a^n|H^r)} $$

is minimized.

\section{Change Zero to Baseline Model}

The difference from my Change Zero to baseline model is in the decoding part. In change zero, I use

$${h_k}^a = H^r\beta _{k}^T.$$

That is, I use attention weights to calculate a weighted average of input states, then use this weighted average to query the next attention weights.

\section{Change One to Baseline Model}

The difference between change one and change zero is removing the $W^r\overrightarrow{{h_{i-1}}^r}$ in bi-directional match-LSTM layer. I am interested in this experiments since I guess the $\overrightarrow{{h_{i-1}}^r}$ might carries some redundant information. After this change,


$$\overrightarrow{G} = tanh(W^qH^q + (W^p{h_i}^p + b^p) \otimes e_q)$$


\section{Change Two to Baseline Model}

The difference between chagne two and change zero is removing the preprocessing layer in encoding process. This is the check whether preprocessing layer carries some redundant information.

\section{Change Three to Baseline Model}

The difference between change three and change zero is both preprocessing layer and $W^r\overrightarrow{{h_{i-1}}^r}$ in bi-directional match-LSTM layer are removed. This is to experiment whether they context information of both is carried in some other place in the change zero model.

\chapter{Implementation}

\section{Adjust Data to Support Batch Training}\label{sect:padding}

In practice, passages have different length. So do questions. However, in one specific model, the number of times to iterate encoding or decoding process is fixed. We are intended to use all training data to train the model instead of using a single instance. As such, adjusting all passages to a same length and adjusting all questions to another same length is necessary. For sequences longer than a fixed length, we cut some part of the sentence out. For sequences shorted than a fixed length, we use some special vector to pad them.

Due to this adjustment, the models in implementation has some difference with the theoretical ones. Below I will talking about the difference taking the baseline model as example.

Assume in data processing step, each passage is adjusted to $passage\_padding\_length$ and is paired with an vector $passage\_mask$ which has size $passage\_padding\_length$ and each question is adjusted to $question\_padding\_length$ and paired with one $question\_mask$ which has size $question\_padding\_length$. The entry of mask vector is either 0 or 1. 0 indicates the current token does not exit in original sequence. 1 indicates the opposite. When constructing the model, every effort is made to prevent the model from distracted by not existing tokens.

In preprocessing layer, after getting a sequence of hidden states, the mask vector is used to reset the value of not existing positions to zero by an additional step
$$H^p = H^p \circ (passage\_mask \otimes l)$$
$$H^q = H^q \circ (question\_mask \otimes l).$$

In match-LSTM layer, the attention weights of not existing tokens are also set to zero by an additional step
$$\overrightarrow{\alpha _i} = softmax( (w^t\overrightarrow{G_i} + b \otimes e_q) ) \circ question\_mask .$$

Similar to preprocessing layer, we have
$$H_r = H_r \circ (passage\_mask \otimes 2l).$$

In Ans-Ptr layer, similar to match-LSTM layer, we have
$$\beta _k = softmax( (v^tF_k + c \otimes e_p) ) \circ passage\_mask.$$


\section{Tensorflow Graph}

Tensorflow is an open source machine learning framework. The central idea of Tensorflow is describing a complex numeric computation as a graph.  {\tt Variables} are "trainable" nodes.  {\tt Placeholders} are nodes whose values are fed in run time. Taking the baseline model as an example,  {\tt Variables} should be used to represent all the parameters of encoding and decoding layers and {\tt Placeholders} should be used to represent passages, questions, and answers. To train a graph, we do not need to compute the gradients on our own. Instead, we call some APIs of Tensorflow to get a train operation, and then feed data through placeholders to the train operation to update the parameters.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/tf_graph.png}
  \caption{Concept of Tensorflow graphs in this project}
  \label{f:tf_graph}
\end{figure}

Figure \ref{f:tf_graph} describes the concept of the Tensorflow graphs I use in this project. The cloud shape represents the {\tt Placeholders} whose values are to be fed in run time. The {\tt Variables} are included in {\tt Encoder} and {\tt Decoder}. The rectangles represent Tensors in the calculation process. The circles represent the final Tensors we use.

Here is how to build the graph. In the embedding step, the model transforms tokens to word feature vectors using a given word feature vector matrix. This matrix is part of the graph is is constant. The {\tt Encoder} encodes vectors to $H_r$ using a lot of {\tt Variables}, and the {\tt Decoder} decoders $H_r$ using a lot of {\tt Variables} to two probability distributions $\beta _0$ and $\beta _1$. Recall that $\beta _0$ tells the likeliness of each token in passage to be the start of the answer and $\bet _1$ tells the likeliness of each token in passage to be the end of the answer. The loss is calculated using $\beta _0$, $\beta _1$ and ground truth answers feed through answer {\tt Placeholders}. From the loss, a train operation is calculated. It includes the gradients on all {\tt Variables}.

Here is how the run time works. In training and validation process, the passages, questions and answers are fed into the graph. Running the train operation with data fed into through {\tt Placeholders} will update all the {\tt Variables}. In testing process, only passages and questions are fed into the graph. The predicted answers are determined from $\beta _0$ and $\beta _1$.






\section{Implementation Pipelines}

The train and validation process produces a brunch of Tensorflow graphs. The validation loss is used to choose the best graph. Then the best graph is used to do testing.

\begin{figure}[htbp]\centering
  \includegraphics[width=11cm, height=3cm]{figures/pipeline.png}
  \caption{Implementation Pipelines}
  \label{f:pipeline}
\end{figure}


\chapter{Experiments}
\section{Data}
I use Stanford Question Answering Dataset (SQuAD) to do experiemnts.  It contains 100,000+ question-answer pairs on 500+ articles. The training set and dev set are visible to users. However, the test set is hidden. To perform my experiments, I split the training set into my training and dev set, and use the dev set as my test set. After such splitting, my training set contains 78839 question-answer pairs, my dev set contains 8760 question-answer pairs, and my test set contains 10570 question-answer pairs.

\begin{table}[htbp]\centering
  \caption{Data Sets}
  \label{tab:dataset}
  \begin{tabular}{|r|l|} \hline
    Set Name & Number of Instances \\ \hline\hline
    Train & \ 78839 \\
    Validation & \ 8760 \\
    Test & \ 10570 \\ \hline
  \end{tabular}
\end{table}

Regarding to data processing, I use a Python natural language processing library {\tt nltk} to tokenize raw strings in json file into passage-question-answer triplets in the format of word token sequences.

Then I face two ways to represent the word tokens. The first way is to query the corresponding word vector of each token from the GloVe embedding matrix and turn the word sequences into vector sequences.  The second way is to make a vocabulary and turn each word sequence to an index sequence based on the vocabulary. At the same time, a smaller embedding matrix is made from the original GloVe embedding matrix. The index of each token in this matrix is same with that in vocabulary. The index sequences and the smaller embedding matrix is fed into the neural network. Since the index sequences require much less memory than the vector sequences, the second way saves a lot of memory than the first way when large data set is used.


The next step is to pad passage and question, as mentioned in Section \ref{sect:padding}. There is a {\tt pad} token in vocabulary, and the corresponding word vector is zero vector.

At last, split data into batches.

After training, the embedding matrix becomes part of the tensorflow graph, and is unchanged during testing. Also, the vocabulary remains unchanged during testing. Words not found in vocabulary during testing are treated as unknown word. The word vector of unknown words is the average of all vectors of known words in my training and dev set.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=9cm]{figures/data.png}
  \caption{Data Pipelines}
  \label{f:data}
\end{figure}

\section{Settings}
At the first step, I refer to the experimental settings in InnerPeace-Wu's Github page to set some of my parameters. The dimension of GloVe word vectors is 100, the size of hidden units is 64, the regularization scale of L2-regularization is 0.001 and the batch size is 32. For passage length and question length, I plot out their distributions and find out that 400 is a reasonable cut point for passage, and 30 is a reasonable cut point for question.


\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/passage_length.png}
  \caption{Passage Length Distribution}
  \label{f:passage_length}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=10cm, height=7cm]{figures/question_length.png}
  \caption{Question Length Distribution}
  \label{f:question_length}
\end{figure}

I use the default settings of tensorflow for adam optimizer. I sample out 200 instances from train set to estimate train error, and from dev set to estimate validation error. The normalization boundary to clip gradients is set as 5.

Then I do several experiments on different learning rate, since this is the most important parameters. I initially planed to do this on the match architecture, but due to some code bugs at that time, I actually do the experiments on match\_change1 architecture. I started the experiments from 0.002, which is used in InnerPeace-Wu's Github page. It performs well. Then I try several larger value. However, all the larger values I try do not perform well. So I set learning rate to 0.002.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/lr.png}
  \caption{Performance of Different Learning Rates}
  \label{f:lr}
\end{figure}

The settings that work well on one architectures also work well on other three architectures. This really sames a lot of time and money.

The dimension of GloVe word vectors is 100, the size of hidden units is 64, the regularization scale of L2-regularization is 0.001 and the batch size is 32. For passage length and question length, I plot out their distributions and find out that 400 is a reasonable cut point for passage, and 30 is a reasonable cut point for question.

\begin{table}[htbp]\centering
  \caption{Experimental Settings}
  \label{tab:dataset}
  \begin{tabular}{|r|l|} \hline
    Hyperparameter Name& Value \\ \hline\hline
    WFC Dimension (d) & \ 100 \\
    Hidden State Size (l) & \ 64 \\
    L2\_regularization Scale & \ 10570\\
    Hidden State Size (l) & \ 64\\
    Batch Size & \ 64\\
    Passage Length & \ 400\\
    Question Length & \ 30\\
    Clip Norm & \ 5\\
    Learning Rate & \ 0.002 \\ \hline
  \end{tabular}
\end{table}


I use F1 score and exact match score to evaluate the performance of each architecture. TODO:explain F1 and em

I used Tesla K80 12 GB Memory 61 GB RAM 100 GB SSD GPU to train the four models.

\section{Results}



describe patterns in trainningã€‚ For match architecture, the model converges after around 8 epochs. Match\_change1 and match\_change2 perform similar to match. For match\_change2, the model converges after around 4 epochs.  including training time, loss change, score change etc. For all four architectures, training each epoch costs around 2 hours on the Tesla GPU I use.

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_corrected.png}
  \caption{Training Process of MLAP Model}
  \label{f:mlap}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_baseline.png}
  \caption{Training Process of Baseline Model}
  \label{f:baseline}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change1.png}
  \caption{Training Process of Baseline Change1 Model}
  \label{f:baseline_change1}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change2.png}
  \caption{Training Process of Baseline Change2 Model}
  \label{f:baseline_change2}
\end{figure}

\begin{figure}[htbp]\centering
  \includegraphics[width=12cm, height=12cm]{figures/match_change3.png}
  \caption{Training Process of Baseline Change3 Model}
  \label{f:baseline_change3}
\end{figure}

describe all results. In the original dev set, which is our test set, each question corresponds to multiple answers. This makes sense since in reality we might have several different ways to answer a same question. We use two ways to calculate the scores. The first way is choose the first answer, the second way is calculating scores for each answer and choose the best score.In either way, match, match\_change1 and match\_change2 perform similarly, but match\_change3 behaves much worse than the other three.


\begin{table}[htbp]\centering
  \caption{Testing Results}
  \label{tab:test_results}
  \begin{tabular}{|c|c|c|}
    \hline
    Model& Test Exact Match & Test F1 \\
    \hline\hline
    MLAP & \ 23.4 &\ 33.6 \\
    Baseline & \ 33.0 &\ 45.8 \\
    Baseline Change1 & \ 33.0 &\ 46.2 \\
    Baseline Change2 & \ 33.0 &\ 45.6 \\
    Baseline Change3 & \ 24.3 &\ 33.9 \\
    \hline
  \end{tabular}
\end{table}

\section{Analysis}
We can draw several interesting insights from the results. First, more context information increases, or at least does not decrease the accuracy. Second, the ways to add context information are various. Third, duplicate context might not be necessary.





\chapter{Conclusion}

In this project, I developed four models for the question answering task using the Stanford Question Answering (SQuAD) dataset based on Wang's match-LSTM and Pointer Network model. I successfully implemented the four models and got reasonable training and testing results. Through comparing the results of three changed models with the baseline model, I also found some redundant design in the baseline model - preprocessing layer and the $h_r$ in bidirecional match-lstm layer provides duplicate context information.

In the future, I plan to research on the opposite direction - adding more context information to the existing model.


% if you want to keep macros or chapters in separate files
% you can do that and include them with \input like this:

%\input macros.tex
%\input ch1.tex
%\input ch2.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%  Bibliography %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{amsalpha}	% or "siam", or "alpha", or "abbrv"
				% see other styles in
				% texmf/bibtex/bst

%\nocite{*}		% uncomment to list all refs in database,
			% cited or not.

\bibliography{refs}		% assumes bib database in "refs.bib"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%  Appendices %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix	% don't forget this line if you have appendices!

% \chapter{Gratuitous Appendix}
% Nothing to see here.

% %\input appA.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%   THE END   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
