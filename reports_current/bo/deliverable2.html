<h1>Word Embedding</h1>
<p>Word embedding is an important language modeling technique. It is usually the first step towards applying neural networks to natural language processing. Understanding word embedding is an essential part of understanding how to use a neural network model to build a Question Answering system.</p>

<p> Word embedding is a way to map each word to a feature vector in a continuous space. The dimension of the continuous space is much lower than the dimension of the one-hot vector, which is comparable to the vocabulary size. Also, the distance between two word feature vectors could tell how likely the two corresponding words appear in a same context.</p>
<p>
Word embedding technique is originally introduced by  Bengio et al in <i> Bengio, Yoshua, et al. "A neural probabilistic language model." Journal of machine learning research 3.Feb (2003): 1137-1155. </i> They proposed a neural probabilistic language model (NPLM) to map each word to a feature vector. The training set is a sequence of words `w_1,...,w_T` where `w_t \in V` and `V` is the vocabulary. The purpose is to train a model `f` such that ` \hat{P}(w_t | w_{t-1},...,w_{t-n+1}) = f(w_t, ..., w_{t-n+1})`. The model `f(w_t, ..., w_{t-n+1})` is divided into two parts. First, map each `w` to a distributed feature vector by selecting the corresponding row in `C` and concatenate the feature vectors to get `x=(C(w_{t-1}),... ,C(w_{t-n+1}))`. Second, map `x` to `f(w_t, ..., w_{t-n+1})` following `y=b+W\cdot x + U\cdot tanh(d + H\cdot x)` and ` f(w_t, ..., w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_{i}^{}e^{y_i}}`. The loss function to minimize is `L = -\frac{1}{T}\sum _{t}^{} \log{f(w_t, ..., w_{t-n+1})}`. After the model is trained, the matrix `C` includes the feature vectors of all words in `V`.
</p>
<p>
At the present time, a simplified architecture Skip-gram proposed by Mikolov et al in <i>Mikolov, Tomas, et al. "Efficient estimation of word representations in vector space." arXiv preprint arXiv:1301.3781 (2013).</i>  is widely used. The main difference between Skip-gram and NPLM is the first one removes tanh layer.
</p>

<p>
I implemented in Python both the <a href="https://github.com/booooooooooooooo/poem_generator/blob/master/classicChinesePoemGenerator/wordEmbeddingNPLM.py"> NPLM model without Noise Contrastive Estimation (NCE) loss </a> and <a href="https://github.com/booooooooooooooo/poem_generator/blob/master/classicChinesePoemGenerator/wordEmbeddingSkipGram.py"> Skip-gram model with NCE loss</a>. I use <a href="http://homepages.inf.ed.ac.uk/mlap/Data/EMNLP14/"> a collection of 284899 classic Chinese poems </a> as the corpus. The data is published by authors of paper <i>"Zhang, Xingxing, and Mirella Lapata. "Chinese Poetry Generation with Recurrent Neural Networks." EMNLP. 2014."</i>
</p>


<p>Here are some experimental results of the Skip-gram model together with negative sampling implementation. Training each epoch took about 8 minutes. After about 5 epochs, the valid loss reached the lowest. Below is a list of some character pairs with high cosine similarity. These characters are from 200 most frequent characters in corpus. According to my knowledge of Chinese classic poems, in many pairs, the two characters have high probability to appear in same context. As such, I think the model is implemented correctly. </p>

<img src="bo/similarities200_partial.png" alt="" style="width:300px;" />
