<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-type" content="text/html; charset=UTF-8" />
</head>

<body>

<h1><center>Word Embedding of Corpus</center></h1>
<h2>The Model</h2>
<p>
I firstly implemented NPLM algorithm in paper<i> Bengio, Yoshua, et al. "A neural probabilistic language model." Journal of machine learning research 3.Feb (2003): 1137-1155. </i> Here is <a href="folder_pdf/NPLM.pdf"> a brief summary of the NPLM model - PDF</a>.
</p>

<p>
Since NPLM runs slowly in my mac book, I implemented skip-gram together with negative sampling in paper <i>Mikolov, Tomas, et al. "Efficient estimation of word representations in vector space." arXiv preprint arXiv:1301.3781 (2013).</i> to decrease the training time. The skip-gram model removes the hidden layer of NPLM model. Negative sampling is a Noise Contrastive Estimation (NCE) loss.
</p>

<h2>Corpus and Implementation</h2>

<p>

</p>

<p>
The corpus I use is a collection of 284899 classic Chinese poems . I get raw data from  <a href="http://homepages.inf.ed.ac.uk/mlap/Data/EMNLP14/"> Link to raw data. </a>.  The data is published by authors of paper "Zhang, Xingxing, and Mirella Lapata. "Chinese Poetry Generation with Recurrent Neural Networks." EMNLP. 2014." They also use the data in their paper. All data is split into train, valid, and test. The valid loss is the indicator for choosing model. The test loss is a cross entropy  intrinsic evaluation.  I also use word similarity as another intrinsic evaluation.
</p>

<p>
I used Python and Tensorflow to implement the model. For details, please refer to <a href="https://github.com/booooooooooooooo/sjsuMasterProject/tree/master/classicChinesePoemGenerator"> Link to my Github repository. </a>
</P>

<h2>Result</h2>

<p>Here are some information about the skip-gram together with negative sampling implementation. Training each epoch costs about 8 minutes. After about 5 epochs, the valid loss reaches the lowest. Below is a list of some character pairs with high cosine similarities. These characters are from 200 most frequent characters in corpus. According to my knowledge of Chinese classic poems, in many pairs, the two characters have high probability to appear in same context. As such, I think the model is implemented correctly. </p>

<img src="folder_image/similarities200_partial" alt="" style="width:300px;" />

</body>

</html>
